{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10,123,4324435])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 123, 4324435]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0 = rdd.map(lambda x : x * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 15129, 18700738069225]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18700738084739"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5cf337af7cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rdd0' is not defined"
     ]
    }
   ],
   "source": [
    "rdd0.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd0.join(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd0.union(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 25,\n",
       " 36,\n",
       " 49,\n",
       " 64,\n",
       " 81,\n",
       " 100,\n",
       " 15129,\n",
       " 18700738069225,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 123,\n",
       " 4324435]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 23, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-75ecb6e3744c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 23, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n"
     ]
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"/opt/spark/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18700738069225"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[20] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-33586ae430ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "rdd1.partitioner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[14] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RDD.id of UnionRDD[14] at union at NativeMethodAccessorImpl.java:0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7548a07a96fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkHome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "sc.sparkHome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = sc.textFile(\"/opt/data/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = txts.flatMap(lambda line : line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = words.map(lambda w : (w,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0 = rdd.reduceByKey(lambda x, y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 12),\n",
       " ('Spark', 6),\n",
       " ('Cluster', 1),\n",
       " ('docker-compose', 3),\n",
       " ('', 104),\n",
       " ('General', 1),\n",
       " ('simple', 2),\n",
       " ('purposses.', 1),\n",
       " ('away', 1),\n",
       " ('solution', 1),\n",
       " ('development', 1),\n",
       " ('The', 10),\n",
       " ('following', 6),\n",
       " ('container|Ip', 1),\n",
       " ('---|---', 1),\n",
       " ('spark-master|10.5.0.2', 1),\n",
       " ('spark-worker-1|10.5.0.3', 1),\n",
       " ('Installation', 1),\n",
       " ('steps', 1),\n",
       " ('make', 10),\n",
       " ('run', 7),\n",
       " ('Pre', 1),\n",
       " ('*', 20),\n",
       " ('installed', 2),\n",
       " ('Jar', 1),\n",
       " ('Build', 1),\n",
       " ('step', 3),\n",
       " ('of', 9),\n",
       " ('these', 3),\n",
       " ('performed', 1),\n",
       " ('*build-images.sh*', 1),\n",
       " ('executions', 1),\n",
       " ('is', 14),\n",
       " ('as', 6),\n",
       " ('build-images.sh', 1),\n",
       " ('docker', 15),\n",
       " ('images:', 1),\n",
       " ('image', 9),\n",
       " ('based', 5),\n",
       " ('ships', 1),\n",
       " ('python3', 1),\n",
       " ('created', 6),\n",
       " ('used', 8),\n",
       " ('master', 8),\n",
       " ('spark-worker:2.3.1:', 1),\n",
       " ('worker', 9),\n",
       " ('spark-submit:2.3.1:', 1),\n",
       " ('driver', 3),\n",
       " ('die', 1),\n",
       " ('test', 2),\n",
       " ('--scale', 1),\n",
       " ('spark-worker=3', 1),\n",
       " ('Validate', 1),\n",
       " ('Just', 1),\n",
       " ('accesing', 1),\n",
       " ('URL.', 1),\n",
       " ('###', 4),\n",
       " ('http://10.5.0.2:8080/', 1),\n",
       " ('![alt', 4),\n",
       " ('text](docs/spark-master.png', 1),\n",
       " ('UI\")', 4),\n",
       " ('1', 5),\n",
       " ('http://10.5.0.3:8081/', 1),\n",
       " ('text](docs/spark-worker-2.png', 1),\n",
       " ('text](docs/spark-worker-3.png', 1),\n",
       " ('Allocation', 1),\n",
       " ('three', 1),\n",
       " ('particular', 1),\n",
       " ('set', 1),\n",
       " ('resource', 1),\n",
       " ('RAM', 5),\n",
       " ('default', 4),\n",
       " ('CPU', 1),\n",
       " ('core.', 1),\n",
       " ('1024', 1),\n",
       " ('256mb.', 1),\n",
       " ('128mb', 1),\n",
       " ('this', 4),\n",
       " ('just', 5),\n",
       " ('edit', 1),\n",
       " ('env/spark-worker.sh', 1),\n",
       " ('file.', 1),\n",
       " ('Volumes', 1),\n",
       " (\"I've\", 1),\n",
       " ('two', 1),\n",
       " ('mounts', 1),\n",
       " ('in', 4),\n",
       " ('chart:', 1),\n",
       " ('Host', 1),\n",
       " ('Mount|Container', 1),\n",
       " ('Mount|Purposse', 1),\n",
       " ('---|---|---', 1),\n",
       " ('jars', 1),\n",
       " ('/mnt/spark-data|/opt/spark-data|', 1),\n",
       " ('basically', 2),\n",
       " ('dummy', 1),\n",
       " ('DFS', 1),\n",
       " ('Volumes...(maybe', 1),\n",
       " ('Now', 1),\n",
       " ('let`s', 1),\n",
       " ('**wild', 1),\n",
       " ('new', 1),\n",
       " ('Create', 1),\n",
       " ('do', 3),\n",
       " ('spark-submit', 5),\n",
       " ('designed', 1),\n",
       " ('scala', 2),\n",
       " ('support', 1),\n",
       " ('guess', 1),\n",
       " ('was', 1),\n",
       " ('am', 1),\n",
       " ('using', 6),\n",
       " ('an', 4),\n",
       " ('[crimes-app](https://).', 1),\n",
       " ('use', 3),\n",
       " ('own', 1),\n",
       " (\"'ve\", 1),\n",
       " ('at', 3),\n",
       " ('hand.', 1),\n",
       " ('Ship', 1),\n",
       " ('jar', 4),\n",
       " ('into', 5),\n",
       " ('workers,', 1),\n",
       " ('configuration', 1),\n",
       " ('input', 2),\n",
       " ('need.', 1),\n",
       " ('Luckily', 1),\n",
       " ('us', 1),\n",
       " ('we', 1),\n",
       " ('are', 3),\n",
       " ('volumes', 1),\n",
       " ('have', 5),\n",
       " ('/mnt/spark-apps,', 1),\n",
       " ('files', 2),\n",
       " ('/mnt/spark-files.', 1),\n",
       " ('#Copy', 2),\n",
       " (\"workers's\", 3),\n",
       " ('cp', 3),\n",
       " ('/home/workspace/crimes-app/build/libs/crimes-app.jar', 1),\n",
       " ('/mnt/spark-apps', 2),\n",
       " ('Copy', 1),\n",
       " ('processed', 1),\n",
       " ('step,', 1),\n",
       " ('curious', 1),\n",
       " ('check', 1),\n",
       " ('before', 1),\n",
       " ('spark-submit.', 1),\n",
       " ('exec', 6),\n",
       " ('spark-worker-1', 2),\n",
       " ('ls', 6),\n",
       " ('/opt/spark-apps', 3),\n",
       " ('spark-worker-2', 2),\n",
       " ('spark-worker-3', 2),\n",
       " ('commands', 1),\n",
       " ('Use', 1),\n",
       " ('command', 1),\n",
       " ('more', 1),\n",
       " ('class', 1),\n",
       " ('#Extra', 1),\n",
       " ('SPARK_SUBMIT_ARGS=\"--conf', 1),\n",
       " ('#We', 1),\n",
       " ('network', 1),\n",
       " ('resolves', 1),\n",
       " ('spark://spark-master:7077)', 1),\n",
       " ('/mnt/spark-apps:/opt/spark-apps', 1),\n",
       " ('SPARK_APPLICATION_JAR_LOCATION=$SPARK_APPLICATION_JAR_LOCATION', 1),\n",
       " ('output', 1),\n",
       " ('pretty', 1),\n",
       " ('like', 1),\n",
       " ('2018-09-23', 6),\n",
       " ('INFO', 6),\n",
       " ('Submission', 1),\n",
       " ('successfully', 2),\n",
       " ('Polling', 1),\n",
       " ('State', 1),\n",
       " ('now', 2),\n",
       " ('RUNNING.', 1),\n",
       " ('Driver', 1),\n",
       " ('worker-20180923151711-10.5.0.4-45381', 1),\n",
       " ('10.5.0.4:45381.', 1),\n",
       " ('driver-20180923151753-0000\",', 1),\n",
       " ('\"submissionId\"', 1),\n",
       " ('true', 1),\n",
       " ('}', 1),\n",
       " ('Summary', 1),\n",
       " ('(What', 1),\n",
       " ('compiled', 1),\n",
       " ('nodes', 1),\n",
       " ('Copied', 1),\n",
       " ('resources', 2),\n",
       " ('Submitted', 1),\n",
       " ('image.', 1),\n",
       " ('ran', 1),\n",
       " ('so).', 1),\n",
       " ('cluster?', 1),\n",
       " ('purposses,', 1),\n",
       " ('way', 1),\n",
       " ('apps', 1),\n",
       " ('laptop', 1),\n",
       " ('desktop.', 1),\n",
       " ('Yarn,', 1),\n",
       " ('Mesos', 1),\n",
       " (':(.', 1),\n",
       " ('useful', 1),\n",
       " ('CI/CD', 1),\n",
       " ('pipelines', 1),\n",
       " ('really', 1),\n",
       " ('hot', 1),\n",
       " ('topic)', 1),\n",
       " ('with', 4),\n",
       " ('Docker', 4),\n",
       " ('&', 6),\n",
       " ('A', 8),\n",
       " ('spark', 27),\n",
       " ('standalone', 3),\n",
       " ('cluster', 9),\n",
       " ('for', 10),\n",
       " ('your', 17),\n",
       " ('testing', 1),\n",
       " ('environment', 3),\n",
       " ('*docker-compose', 1),\n",
       " ('up*', 1),\n",
       " ('from', 2),\n",
       " ('you', 10),\n",
       " ('environment.', 1),\n",
       " ('compose', 3),\n",
       " ('will', 8),\n",
       " ('create', 6),\n",
       " ('the', 34),\n",
       " ('containers:', 1),\n",
       " ('address', 1),\n",
       " ('spark-worker-2|10.5.0.4', 1),\n",
       " ('spark-worker-3|10.5.0.5', 1),\n",
       " (\"cluster's\", 1),\n",
       " ('containers.', 4),\n",
       " ('##', 8),\n",
       " ('requisites', 1),\n",
       " ('Application', 1),\n",
       " ('to', 31),\n",
       " ('play', 1),\n",
       " ('with(Optional)', 1),\n",
       " ('images', 2),\n",
       " ('first', 2),\n",
       " ('deploy', 1),\n",
       " ('be', 6),\n",
       " ('build', 1),\n",
       " ('custom', 1),\n",
       " ('images,', 1),\n",
       " ('builds', 1),\n",
       " ('can', 3),\n",
       " ('script.', 1),\n",
       " ('steps:', 2),\n",
       " ('```sh', 3),\n",
       " ('chmod', 1),\n",
       " ('+x', 1),\n",
       " ('./build-images.sh', 1),\n",
       " ('```', 6),\n",
       " ('This', 6),\n",
       " ('spark-base:2.3.1:', 1),\n",
       " ('base', 1),\n",
       " ('on', 10),\n",
       " ('java:alpine-jdk-8', 1),\n",
       " ('wich', 1),\n",
       " ('scala,', 1),\n",
       " ('and', 13),\n",
       " ('2.3.1', 1),\n",
       " ('spark-master:2.3.1:', 1),\n",
       " ('previously', 3),\n",
       " ('image,', 3),\n",
       " ('a', 18),\n",
       " ('submit', 2),\n",
       " ('containers(run,', 1),\n",
       " ('deliver', 1),\n",
       " ('gracefully).', 1),\n",
       " ('Run', 2),\n",
       " ('final', 1),\n",
       " ('file:', 1),\n",
       " ('up', 1),\n",
       " ('validate', 2),\n",
       " ('UI', 1),\n",
       " ('each', 4),\n",
       " ('Master', 2),\n",
       " ('\"Spark', 4),\n",
       " ('Worker', 6),\n",
       " ('text](docs/spark-worker-1.png', 1),\n",
       " ('2', 3),\n",
       " ('http://10.5.0.4:8081/', 1),\n",
       " ('3', 4),\n",
       " ('http://10.5.0.5:8081/', 1),\n",
       " ('Resource', 1),\n",
       " ('shipped', 2),\n",
       " ('workers', 3),\n",
       " ('one', 3),\n",
       " ('master,', 1),\n",
       " ('has', 1),\n",
       " ('allocation(basically', 1),\n",
       " ('cpu', 2),\n",
       " ('cores', 3),\n",
       " ('allocation).', 1),\n",
       " ('allocation', 3),\n",
       " ('spark-worker', 1),\n",
       " ('MB.', 1),\n",
       " ('executors', 1),\n",
       " ('If', 1),\n",
       " ('wish', 1),\n",
       " ('modify', 1),\n",
       " ('allocations', 1),\n",
       " ('Binded', 1),\n",
       " ('To', 1),\n",
       " ('app', 8),\n",
       " ('running', 6),\n",
       " ('easier', 1),\n",
       " ('volume', 1),\n",
       " ('described', 1),\n",
       " ('/mnt/spark-apps|/opt/spark-apps|Used', 1),\n",
       " ('available', 2),\n",
       " (\"app's\", 3),\n",
       " ('all', 6),\n",
       " ('Used', 1),\n",
       " ('data', 3),\n",
       " ('not...)', 1),\n",
       " ('sample', 2),\n",
       " ('application', 8),\n",
       " ('submit**', 1),\n",
       " ('distributed', 3),\n",
       " ('nature', 1),\n",
       " ('our', 1),\n",
       " ('toy', 1),\n",
       " ('Scala', 1),\n",
       " ('thing', 1),\n",
       " ('need', 2),\n",
       " ('application.', 2),\n",
       " ('Our', 1),\n",
       " ('code', 2),\n",
       " ('(soon', 1),\n",
       " ('ship', 1),\n",
       " ('pyspark', 1),\n",
       " ('I', 6),\n",
       " ('lazy', 1),\n",
       " ('so..).', 1),\n",
       " ('In', 1),\n",
       " ('my', 1),\n",
       " ('case', 1),\n",
       " ('called', 1),\n",
       " ('You', 1),\n",
       " ('or', 4),\n",
       " ('app,', 1),\n",
       " ('because', 1),\n",
       " ('had', 1),\n",
       " ('it', 1),\n",
       " ('dependencies', 1),\n",
       " ('Workers', 1),\n",
       " ('necesary', 1),\n",
       " ('**spark-submit**', 2),\n",
       " ('copy', 3),\n",
       " ('bundle', 1),\n",
       " ('also', 1),\n",
       " ('any', 1),\n",
       " ('file', 3),\n",
       " ('so,', 1),\n",
       " ('configs', 2),\n",
       " ('```bash', 3),\n",
       " ('folder', 3),\n",
       " ('-r', 1),\n",
       " ('/home/workspace/crimes-app/config', 1),\n",
       " ('/home/Crimes_-_2001_to_present.csv', 1),\n",
       " ('/mnt/spark-files', 1),\n",
       " ('Check', 1),\n",
       " ('successful', 1),\n",
       " ('(Optional)', 1),\n",
       " ('not', 1),\n",
       " ('necessary', 3),\n",
       " ('if', 2),\n",
       " ('place', 1),\n",
       " ('Validations', 3),\n",
       " ('-ti', 6),\n",
       " ('-l', 6),\n",
       " ('/opt/spark-data', 3),\n",
       " ('After', 2),\n",
       " ('see', 2),\n",
       " ('files.', 1),\n",
       " ('#Creating', 1),\n",
       " ('some', 1),\n",
       " ('variables', 1),\n",
       " ('readable', 1),\n",
       " ('#App', 2),\n",
       " ('by', 3),\n",
       " ('SPARK_APPLICATION_JAR_LOCATION=\"/opt/spark-apps/crimes-app.jar\"', 1),\n",
       " ('main', 1),\n",
       " ('SPARK_APPLICATION_MAIN_CLASS=\"org.mvb.applications.CrimesApp\"', 1),\n",
       " ('args', 1),\n",
       " ('spark.executor.extraJavaOptions=\\'-Dconfig-path=/opt/spark-apps/dev/config.conf\\'\"',\n",
       "  1),\n",
       " ('same', 1),\n",
       " ('cluster(internally', 1),\n",
       " ('--network', 1),\n",
       " ('docker-spark-cluster_spark-network', 1),\n",
       " ('\\\\', 4),\n",
       " ('-v', 1),\n",
       " ('--env', 2),\n",
       " ('SPARK_APPLICATION_MAIN_CLASS=$SPARK_APPLICATION_MAIN_CLASS', 1),\n",
       " ('spark-submit:2.3.1', 1),\n",
       " ('much', 1),\n",
       " ('this:', 1),\n",
       " ('Running', 1),\n",
       " ('REST', 1),\n",
       " ('submission', 3),\n",
       " ('protocol.', 1),\n",
       " ('15:17:52', 1),\n",
       " ('RestSubmissionClient:54', 6),\n",
       " ('-', 6),\n",
       " ('Submitting', 2),\n",
       " ('request', 2),\n",
       " ('launch', 1),\n",
       " ('spark://spark-master:6066.', 2),\n",
       " ('15:17:53', 5),\n",
       " ('driver-20180923151753-0000.', 1),\n",
       " ('state...', 1),\n",
       " ('status', 1),\n",
       " ('driver-20180923151753-0000', 2),\n",
       " ('Server', 1),\n",
       " ('responded', 1),\n",
       " ('CreateSubmissionResponse:', 1),\n",
       " ('{', 1),\n",
       " ('\"action\"', 1),\n",
       " (':', 5),\n",
       " ('\"CreateSubmissionResponse\",', 1),\n",
       " ('\"message\"', 1),\n",
       " ('\"Driver', 1),\n",
       " ('submitted', 1),\n",
       " ('\"serverSparkVersion\"', 1),\n",
       " ('\"2.3.1\",', 1),\n",
       " ('\"driver-20180923151753-0000\",', 1),\n",
       " ('\"success\"', 1),\n",
       " ('done', 1),\n",
       " (':O?)', 1),\n",
       " ('We', 3),\n",
       " ('node', 1),\n",
       " ('&&', 1),\n",
       " ('docker-compose.', 1),\n",
       " ('home(just', 1),\n",
       " ('enough', 2),\n",
       " ('Why', 1),\n",
       " ('intended', 1),\n",
       " ('Right', 1),\n",
       " (\"don't\", 1),\n",
       " ('Kubernetes', 1),\n",
       " ('apps(A', 1),\n",
       " ('difficult', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd0.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 104),\n",
       " ('![alt', 4),\n",
       " ('\"2.3.1\",', 1),\n",
       " ('\"CreateSubmissionResponse\",', 1),\n",
       " ('\"Driver', 1),\n",
       " ('\"Spark', 4),\n",
       " ('\"action\"', 1),\n",
       " ('\"driver-20180923151753-0000\",', 1),\n",
       " ('\"message\"', 1),\n",
       " ('\"serverSparkVersion\"', 1),\n",
       " ('\"submissionId\"', 1),\n",
       " ('\"success\"', 1),\n",
       " ('#', 12),\n",
       " ('##', 8),\n",
       " ('###', 4),\n",
       " ('#App', 2),\n",
       " ('#Copy', 2),\n",
       " ('#Creating', 1),\n",
       " ('#Extra', 1),\n",
       " ('#We', 1),\n",
       " ('&', 6),\n",
       " ('&&', 1),\n",
       " (\"'ve\", 1),\n",
       " ('(Optional)', 1),\n",
       " ('(What', 1),\n",
       " ('(soon', 1),\n",
       " ('*', 20),\n",
       " ('**spark-submit**', 2),\n",
       " ('**wild', 1),\n",
       " ('*build-images.sh*', 1),\n",
       " ('*docker-compose', 1),\n",
       " ('+x', 1),\n",
       " ('-', 6),\n",
       " ('---|---', 1),\n",
       " ('---|---|---', 1),\n",
       " ('--env', 2),\n",
       " ('--network', 1),\n",
       " ('--scale', 1),\n",
       " ('-l', 6),\n",
       " ('-r', 1),\n",
       " ('-ti', 6),\n",
       " ('-v', 1),\n",
       " ('./build-images.sh', 1),\n",
       " ('/home/Crimes_-_2001_to_present.csv', 1),\n",
       " ('/home/workspace/crimes-app/build/libs/crimes-app.jar', 1),\n",
       " ('/home/workspace/crimes-app/config', 1),\n",
       " ('/mnt/spark-apps', 2),\n",
       " ('/mnt/spark-apps,', 1),\n",
       " ('/mnt/spark-apps:/opt/spark-apps', 1),\n",
       " ('/mnt/spark-apps|/opt/spark-apps|Used', 1),\n",
       " ('/mnt/spark-data|/opt/spark-data|', 1),\n",
       " ('/mnt/spark-files', 1),\n",
       " ('/mnt/spark-files.', 1),\n",
       " ('/opt/spark-apps', 3),\n",
       " ('/opt/spark-data', 3),\n",
       " ('1', 5),\n",
       " ('10.5.0.4:45381.', 1),\n",
       " ('1024', 1),\n",
       " ('128mb', 1),\n",
       " ('15:17:52', 1),\n",
       " ('15:17:53', 5),\n",
       " ('2', 3),\n",
       " ('2.3.1', 1),\n",
       " ('2018-09-23', 6),\n",
       " ('256mb.', 1),\n",
       " ('3', 4),\n",
       " (':', 5),\n",
       " (':(.', 1),\n",
       " (':O?)', 1),\n",
       " ('A', 8),\n",
       " ('After', 2),\n",
       " ('Allocation', 1),\n",
       " ('Application', 1),\n",
       " ('Binded', 1),\n",
       " ('Build', 1),\n",
       " ('CI/CD', 1),\n",
       " ('CPU', 1),\n",
       " ('Check', 1),\n",
       " ('Cluster', 1),\n",
       " ('Copied', 1),\n",
       " ('Copy', 1),\n",
       " ('Create', 1),\n",
       " ('CreateSubmissionResponse:', 1),\n",
       " ('DFS', 1),\n",
       " ('Docker', 4),\n",
       " ('Driver', 1),\n",
       " ('General', 1),\n",
       " ('Host', 1),\n",
       " ('I', 6),\n",
       " (\"I've\", 1),\n",
       " ('INFO', 6),\n",
       " ('If', 1),\n",
       " ('In', 1),\n",
       " ('Installation', 1),\n",
       " ('Jar', 1),\n",
       " ('Just', 1),\n",
       " ('Kubernetes', 1),\n",
       " ('Luckily', 1),\n",
       " ('MB.', 1),\n",
       " ('Master', 2),\n",
       " ('Mesos', 1),\n",
       " ('Mount|Container', 1),\n",
       " ('Mount|Purposse', 1),\n",
       " ('Now', 1),\n",
       " ('Our', 1),\n",
       " ('Polling', 1),\n",
       " ('Pre', 1),\n",
       " ('RAM', 5),\n",
       " ('REST', 1),\n",
       " ('RUNNING.', 1),\n",
       " ('Resource', 1),\n",
       " ('RestSubmissionClient:54', 6),\n",
       " ('Right', 1),\n",
       " ('Run', 2),\n",
       " ('Running', 1),\n",
       " ('SPARK_APPLICATION_JAR_LOCATION=\"/opt/spark-apps/crimes-app.jar\"', 1),\n",
       " ('SPARK_APPLICATION_JAR_LOCATION=$SPARK_APPLICATION_JAR_LOCATION', 1),\n",
       " ('SPARK_APPLICATION_MAIN_CLASS=\"org.mvb.applications.CrimesApp\"', 1),\n",
       " ('SPARK_APPLICATION_MAIN_CLASS=$SPARK_APPLICATION_MAIN_CLASS', 1),\n",
       " ('SPARK_SUBMIT_ARGS=\"--conf', 1),\n",
       " ('Scala', 1),\n",
       " ('Server', 1),\n",
       " ('Ship', 1),\n",
       " ('Spark', 6),\n",
       " ('State', 1),\n",
       " ('Submission', 1),\n",
       " ('Submitted', 1),\n",
       " ('Submitting', 2),\n",
       " ('Summary', 1),\n",
       " ('The', 10),\n",
       " ('This', 6),\n",
       " ('To', 1),\n",
       " ('UI', 1),\n",
       " ('UI\")', 4),\n",
       " ('URL.', 1),\n",
       " ('Use', 1),\n",
       " ('Used', 1),\n",
       " ('Validate', 1),\n",
       " ('Validations', 3),\n",
       " ('Volumes', 1),\n",
       " ('Volumes...(maybe', 1),\n",
       " ('We', 3),\n",
       " ('Why', 1),\n",
       " ('Worker', 6),\n",
       " ('Workers', 1),\n",
       " ('Yarn,', 1),\n",
       " ('You', 1),\n",
       " ('[crimes-app](https://).', 1),\n",
       " ('\\\\', 4),\n",
       " ('```', 6),\n",
       " ('```bash', 3),\n",
       " ('```sh', 3),\n",
       " ('a', 18),\n",
       " ('accesing', 1),\n",
       " ('address', 1),\n",
       " ('all', 6),\n",
       " ('allocation', 3),\n",
       " ('allocation(basically', 1),\n",
       " ('allocation).', 1),\n",
       " ('allocations', 1),\n",
       " ('also', 1),\n",
       " ('am', 1),\n",
       " ('an', 4),\n",
       " ('and', 13),\n",
       " ('any', 1),\n",
       " ('app', 8),\n",
       " (\"app's\", 3),\n",
       " ('app,', 1),\n",
       " ('application', 8),\n",
       " ('application.', 2),\n",
       " ('apps', 1),\n",
       " ('apps(A', 1),\n",
       " ('are', 3),\n",
       " ('args', 1),\n",
       " ('as', 6),\n",
       " ('at', 3),\n",
       " ('available', 2),\n",
       " ('away', 1),\n",
       " ('base', 1),\n",
       " ('based', 5),\n",
       " ('basically', 2),\n",
       " ('be', 6),\n",
       " ('because', 1),\n",
       " ('before', 1),\n",
       " ('build', 1),\n",
       " ('build-images.sh', 1),\n",
       " ('builds', 1),\n",
       " ('bundle', 1),\n",
       " ('by', 3),\n",
       " ('called', 1),\n",
       " ('can', 3),\n",
       " ('case', 1),\n",
       " ('chart:', 1),\n",
       " ('check', 1),\n",
       " ('chmod', 1),\n",
       " ('class', 1),\n",
       " ('cluster', 9),\n",
       " (\"cluster's\", 1),\n",
       " ('cluster(internally', 1),\n",
       " ('cluster?', 1),\n",
       " ('code', 2),\n",
       " ('command', 1),\n",
       " ('commands', 1),\n",
       " ('compiled', 1),\n",
       " ('compose', 3),\n",
       " ('configs', 2),\n",
       " ('configuration', 1),\n",
       " ('containers(run,', 1),\n",
       " ('containers.', 4),\n",
       " ('containers:', 1),\n",
       " ('container|Ip', 1),\n",
       " ('copy', 3),\n",
       " ('core.', 1),\n",
       " ('cores', 3),\n",
       " ('cp', 3),\n",
       " ('cpu', 2),\n",
       " ('create', 6),\n",
       " ('created', 6),\n",
       " ('curious', 1),\n",
       " ('custom', 1),\n",
       " ('data', 3),\n",
       " ('default', 4),\n",
       " ('deliver', 1),\n",
       " ('dependencies', 1),\n",
       " ('deploy', 1),\n",
       " ('described', 1),\n",
       " ('designed', 1),\n",
       " ('desktop.', 1),\n",
       " ('development', 1),\n",
       " ('die', 1),\n",
       " ('difficult', 1),\n",
       " ('distributed', 3),\n",
       " ('do', 3),\n",
       " ('docker', 15),\n",
       " ('docker-compose', 3),\n",
       " ('docker-compose.', 1),\n",
       " ('docker-spark-cluster_spark-network', 1),\n",
       " (\"don't\", 1),\n",
       " ('done', 1),\n",
       " ('driver', 3),\n",
       " ('driver-20180923151753-0000', 2),\n",
       " ('driver-20180923151753-0000\",', 1),\n",
       " ('driver-20180923151753-0000.', 1),\n",
       " ('dummy', 1),\n",
       " ('each', 4),\n",
       " ('easier', 1),\n",
       " ('edit', 1),\n",
       " ('enough', 2),\n",
       " ('env/spark-worker.sh', 1),\n",
       " ('environment', 3),\n",
       " ('environment.', 1),\n",
       " ('exec', 6),\n",
       " ('executions', 1),\n",
       " ('executors', 1),\n",
       " ('file', 3),\n",
       " ('file.', 1),\n",
       " ('file:', 1),\n",
       " ('files', 2),\n",
       " ('files.', 1),\n",
       " ('final', 1),\n",
       " ('first', 2),\n",
       " ('folder', 3),\n",
       " ('following', 6),\n",
       " ('for', 10),\n",
       " ('from', 2),\n",
       " ('gracefully).', 1),\n",
       " ('guess', 1),\n",
       " ('had', 1),\n",
       " ('hand.', 1),\n",
       " ('has', 1),\n",
       " ('have', 5),\n",
       " ('home(just', 1),\n",
       " ('hot', 1),\n",
       " ('http://10.5.0.2:8080/', 1),\n",
       " ('http://10.5.0.3:8081/', 1),\n",
       " ('http://10.5.0.4:8081/', 1),\n",
       " ('http://10.5.0.5:8081/', 1),\n",
       " ('if', 2),\n",
       " ('image', 9),\n",
       " ('image,', 3),\n",
       " ('image.', 1),\n",
       " ('images', 2),\n",
       " ('images,', 1),\n",
       " ('images:', 1),\n",
       " ('in', 4),\n",
       " ('input', 2),\n",
       " ('installed', 2),\n",
       " ('intended', 1),\n",
       " ('into', 5),\n",
       " ('is', 14),\n",
       " ('it', 1),\n",
       " ('jar', 4),\n",
       " ('jars', 1),\n",
       " ('java:alpine-jdk-8', 1),\n",
       " ('just', 5),\n",
       " ('laptop', 1),\n",
       " ('launch', 1),\n",
       " ('lazy', 1),\n",
       " ('let`s', 1),\n",
       " ('like', 1),\n",
       " ('ls', 6),\n",
       " ('main', 1),\n",
       " ('make', 10),\n",
       " ('master', 8),\n",
       " ('master,', 1),\n",
       " ('modify', 1),\n",
       " ('more', 1),\n",
       " ('mounts', 1),\n",
       " ('much', 1),\n",
       " ('my', 1),\n",
       " ('nature', 1),\n",
       " ('necesary', 1),\n",
       " ('necessary', 3),\n",
       " ('need', 2),\n",
       " ('need.', 1),\n",
       " ('network', 1),\n",
       " ('new', 1),\n",
       " ('node', 1),\n",
       " ('nodes', 1),\n",
       " ('not', 1),\n",
       " ('not...)', 1),\n",
       " ('now', 2),\n",
       " ('of', 9),\n",
       " ('on', 10),\n",
       " ('one', 3),\n",
       " ('or', 4),\n",
       " ('our', 1),\n",
       " ('output', 1),\n",
       " ('own', 1),\n",
       " ('particular', 1),\n",
       " ('performed', 1),\n",
       " ('pipelines', 1),\n",
       " ('place', 1),\n",
       " ('play', 1),\n",
       " ('pretty', 1),\n",
       " ('previously', 3),\n",
       " ('processed', 1),\n",
       " ('protocol.', 1),\n",
       " ('purposses,', 1),\n",
       " ('purposses.', 1),\n",
       " ('pyspark', 1),\n",
       " ('python3', 1),\n",
       " ('ran', 1),\n",
       " ('readable', 1),\n",
       " ('really', 1),\n",
       " ('request', 2),\n",
       " ('requisites', 1),\n",
       " ('resolves', 1),\n",
       " ('resource', 1),\n",
       " ('resources', 2),\n",
       " ('responded', 1),\n",
       " ('run', 7),\n",
       " ('running', 6),\n",
       " ('same', 1),\n",
       " ('sample', 2),\n",
       " ('scala', 2),\n",
       " ('scala,', 1),\n",
       " ('script.', 1),\n",
       " ('see', 2),\n",
       " ('set', 1),\n",
       " ('ship', 1),\n",
       " ('shipped', 2),\n",
       " ('ships', 1),\n",
       " ('simple', 2),\n",
       " ('so).', 1),\n",
       " ('so,', 1),\n",
       " ('so..).', 1),\n",
       " ('solution', 1),\n",
       " ('some', 1),\n",
       " ('spark', 27),\n",
       " ('spark-base:2.3.1:', 1),\n",
       " ('spark-master:2.3.1:', 1),\n",
       " ('spark-master|10.5.0.2', 1),\n",
       " ('spark-submit', 5),\n",
       " ('spark-submit.', 1),\n",
       " ('spark-submit:2.3.1', 1),\n",
       " ('spark-submit:2.3.1:', 1),\n",
       " ('spark-worker', 1),\n",
       " ('spark-worker-1', 2),\n",
       " ('spark-worker-1|10.5.0.3', 1),\n",
       " ('spark-worker-2', 2),\n",
       " ('spark-worker-2|10.5.0.4', 1),\n",
       " ('spark-worker-3', 2),\n",
       " ('spark-worker-3|10.5.0.5', 1),\n",
       " ('spark-worker:2.3.1:', 1),\n",
       " ('spark-worker=3', 1),\n",
       " ('spark.executor.extraJavaOptions=\\'-Dconfig-path=/opt/spark-apps/dev/config.conf\\'\"',\n",
       "  1),\n",
       " ('spark://spark-master:6066.', 2),\n",
       " ('spark://spark-master:7077)', 1),\n",
       " ('standalone', 3),\n",
       " ('state...', 1),\n",
       " ('status', 1),\n",
       " ('step', 3),\n",
       " ('step,', 1),\n",
       " ('steps', 1),\n",
       " ('steps:', 2),\n",
       " ('submission', 3),\n",
       " ('submit', 2),\n",
       " ('submit**', 1),\n",
       " ('submitted', 1),\n",
       " ('successful', 1),\n",
       " ('successfully', 2),\n",
       " ('support', 1),\n",
       " ('test', 2),\n",
       " ('testing', 1),\n",
       " ('text](docs/spark-master.png', 1),\n",
       " ('text](docs/spark-worker-1.png', 1),\n",
       " ('text](docs/spark-worker-2.png', 1),\n",
       " ('text](docs/spark-worker-3.png', 1),\n",
       " ('the', 34),\n",
       " ('these', 3),\n",
       " ('thing', 1),\n",
       " ('this', 4),\n",
       " ('this:', 1),\n",
       " ('three', 1),\n",
       " ('to', 31),\n",
       " ('topic)', 1),\n",
       " ('toy', 1),\n",
       " ('true', 1),\n",
       " ('two', 1),\n",
       " ('up', 1),\n",
       " ('up*', 1),\n",
       " ('us', 1),\n",
       " ('use', 3),\n",
       " ('used', 8),\n",
       " ('useful', 1),\n",
       " ('using', 6),\n",
       " ('validate', 2),\n",
       " ('variables', 1),\n",
       " ('volume', 1),\n",
       " ('volumes', 1),\n",
       " ('was', 1),\n",
       " ('way', 1),\n",
       " ('we', 1),\n",
       " ('wich', 1),\n",
       " ('will', 8),\n",
       " ('wish', 1),\n",
       " ('with', 4),\n",
       " ('with(Optional)', 1),\n",
       " ('worker', 9),\n",
       " ('worker-20180923151711-10.5.0.4-45381', 1),\n",
       " ('workers', 3),\n",
       " (\"workers's\", 3),\n",
       " ('workers,', 1),\n",
       " ('you', 10),\n",
       " ('your', 17),\n",
       " ('{', 1),\n",
       " ('}', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda t :(t[1],t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[39] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(104, ''),\n",
       " (4, '![alt'),\n",
       " (1, '\"2.3.1\",'),\n",
       " (1, '\"CreateSubmissionResponse\",'),\n",
       " (1, '\"Driver'),\n",
       " (4, '\"Spark'),\n",
       " (1, '\"action\"'),\n",
       " (1, '\"driver-20180923151753-0000\",'),\n",
       " (1, '\"message\"'),\n",
       " (1, '\"serverSparkVersion\"'),\n",
       " (1, '\"submissionId\"'),\n",
       " (1, '\"success\"'),\n",
       " (12, '#'),\n",
       " (8, '##'),\n",
       " (4, '###'),\n",
       " (2, '#App'),\n",
       " (2, '#Copy'),\n",
       " (1, '#Creating'),\n",
       " (1, '#Extra'),\n",
       " (1, '#We'),\n",
       " (6, '&'),\n",
       " (1, '&&'),\n",
       " (1, \"'ve\"),\n",
       " (1, '(Optional)'),\n",
       " (1, '(What'),\n",
       " (1, '(soon'),\n",
       " (20, '*'),\n",
       " (2, '**spark-submit**'),\n",
       " (1, '**wild'),\n",
       " (1, '*build-images.sh*'),\n",
       " (1, '*docker-compose'),\n",
       " (1, '+x'),\n",
       " (6, '-'),\n",
       " (1, '---|---'),\n",
       " (1, '---|---|---'),\n",
       " (2, '--env'),\n",
       " (1, '--network'),\n",
       " (1, '--scale'),\n",
       " (6, '-l'),\n",
       " (1, '-r'),\n",
       " (6, '-ti'),\n",
       " (1, '-v'),\n",
       " (1, './build-images.sh'),\n",
       " (1, '/home/Crimes_-_2001_to_present.csv'),\n",
       " (1, '/home/workspace/crimes-app/build/libs/crimes-app.jar'),\n",
       " (1, '/home/workspace/crimes-app/config'),\n",
       " (2, '/mnt/spark-apps'),\n",
       " (1, '/mnt/spark-apps,'),\n",
       " (1, '/mnt/spark-apps:/opt/spark-apps'),\n",
       " (1, '/mnt/spark-apps|/opt/spark-apps|Used'),\n",
       " (1, '/mnt/spark-data|/opt/spark-data|'),\n",
       " (1, '/mnt/spark-files'),\n",
       " (1, '/mnt/spark-files.'),\n",
       " (3, '/opt/spark-apps'),\n",
       " (3, '/opt/spark-data'),\n",
       " (5, '1'),\n",
       " (1, '10.5.0.4:45381.'),\n",
       " (1, '1024'),\n",
       " (1, '128mb'),\n",
       " (1, '15:17:52'),\n",
       " (5, '15:17:53'),\n",
       " (3, '2'),\n",
       " (1, '2.3.1'),\n",
       " (6, '2018-09-23'),\n",
       " (1, '256mb.'),\n",
       " (4, '3'),\n",
       " (5, ':'),\n",
       " (1, ':(.'),\n",
       " (1, ':O?)'),\n",
       " (8, 'A'),\n",
       " (2, 'After'),\n",
       " (1, 'Allocation'),\n",
       " (1, 'Application'),\n",
       " (1, 'Binded'),\n",
       " (1, 'Build'),\n",
       " (1, 'CI/CD'),\n",
       " (1, 'CPU'),\n",
       " (1, 'Check'),\n",
       " (1, 'Cluster'),\n",
       " (1, 'Copied'),\n",
       " (1, 'Copy'),\n",
       " (1, 'Create'),\n",
       " (1, 'CreateSubmissionResponse:'),\n",
       " (1, 'DFS'),\n",
       " (4, 'Docker'),\n",
       " (1, 'Driver'),\n",
       " (1, 'General'),\n",
       " (1, 'Host'),\n",
       " (6, 'I'),\n",
       " (1, \"I've\"),\n",
       " (6, 'INFO'),\n",
       " (1, 'If'),\n",
       " (1, 'In'),\n",
       " (1, 'Installation'),\n",
       " (1, 'Jar'),\n",
       " (1, 'Just'),\n",
       " (1, 'Kubernetes'),\n",
       " (1, 'Luckily'),\n",
       " (1, 'MB.'),\n",
       " (2, 'Master'),\n",
       " (1, 'Mesos'),\n",
       " (1, 'Mount|Container'),\n",
       " (1, 'Mount|Purposse'),\n",
       " (1, 'Now'),\n",
       " (1, 'Our'),\n",
       " (1, 'Polling'),\n",
       " (1, 'Pre'),\n",
       " (5, 'RAM'),\n",
       " (1, 'REST'),\n",
       " (1, 'RUNNING.'),\n",
       " (1, 'Resource'),\n",
       " (6, 'RestSubmissionClient:54'),\n",
       " (1, 'Right'),\n",
       " (2, 'Run'),\n",
       " (1, 'Running'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=\"/opt/spark-apps/crimes-app.jar\"'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=$SPARK_APPLICATION_JAR_LOCATION'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=\"org.mvb.applications.CrimesApp\"'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=$SPARK_APPLICATION_MAIN_CLASS'),\n",
       " (1, 'SPARK_SUBMIT_ARGS=\"--conf'),\n",
       " (1, 'Scala'),\n",
       " (1, 'Server'),\n",
       " (1, 'Ship'),\n",
       " (6, 'Spark'),\n",
       " (1, 'State'),\n",
       " (1, 'Submission'),\n",
       " (1, 'Submitted'),\n",
       " (2, 'Submitting'),\n",
       " (1, 'Summary'),\n",
       " (10, 'The'),\n",
       " (6, 'This'),\n",
       " (1, 'To'),\n",
       " (1, 'UI'),\n",
       " (4, 'UI\")'),\n",
       " (1, 'URL.'),\n",
       " (1, 'Use'),\n",
       " (1, 'Used'),\n",
       " (1, 'Validate'),\n",
       " (3, 'Validations'),\n",
       " (1, 'Volumes'),\n",
       " (1, 'Volumes...(maybe'),\n",
       " (3, 'We'),\n",
       " (1, 'Why'),\n",
       " (6, 'Worker'),\n",
       " (1, 'Workers'),\n",
       " (1, 'Yarn,'),\n",
       " (1, 'You'),\n",
       " (1, '[crimes-app](https://).'),\n",
       " (4, '\\\\'),\n",
       " (6, '```'),\n",
       " (3, '```bash'),\n",
       " (3, '```sh'),\n",
       " (18, 'a'),\n",
       " (1, 'accesing'),\n",
       " (1, 'address'),\n",
       " (6, 'all'),\n",
       " (3, 'allocation'),\n",
       " (1, 'allocation(basically'),\n",
       " (1, 'allocation).'),\n",
       " (1, 'allocations'),\n",
       " (1, 'also'),\n",
       " (1, 'am'),\n",
       " (4, 'an'),\n",
       " (13, 'and'),\n",
       " (1, 'any'),\n",
       " (8, 'app'),\n",
       " (3, \"app's\"),\n",
       " (1, 'app,'),\n",
       " (8, 'application'),\n",
       " (2, 'application.'),\n",
       " (1, 'apps'),\n",
       " (1, 'apps(A'),\n",
       " (3, 'are'),\n",
       " (1, 'args'),\n",
       " (6, 'as'),\n",
       " (3, 'at'),\n",
       " (2, 'available'),\n",
       " (1, 'away'),\n",
       " (1, 'base'),\n",
       " (5, 'based'),\n",
       " (2, 'basically'),\n",
       " (6, 'be'),\n",
       " (1, 'because'),\n",
       " (1, 'before'),\n",
       " (1, 'build'),\n",
       " (1, 'build-images.sh'),\n",
       " (1, 'builds'),\n",
       " (1, 'bundle'),\n",
       " (3, 'by'),\n",
       " (1, 'called'),\n",
       " (3, 'can'),\n",
       " (1, 'case'),\n",
       " (1, 'chart:'),\n",
       " (1, 'check'),\n",
       " (1, 'chmod'),\n",
       " (1, 'class'),\n",
       " (9, 'cluster'),\n",
       " (1, \"cluster's\"),\n",
       " (1, 'cluster(internally'),\n",
       " (1, 'cluster?'),\n",
       " (2, 'code'),\n",
       " (1, 'command'),\n",
       " (1, 'commands'),\n",
       " (1, 'compiled'),\n",
       " (3, 'compose'),\n",
       " (2, 'configs'),\n",
       " (1, 'configuration'),\n",
       " (1, 'containers(run,'),\n",
       " (4, 'containers.'),\n",
       " (1, 'containers:'),\n",
       " (1, 'container|Ip'),\n",
       " (3, 'copy'),\n",
       " (1, 'core.'),\n",
       " (3, 'cores'),\n",
       " (3, 'cp'),\n",
       " (2, 'cpu'),\n",
       " (6, 'create'),\n",
       " (6, 'created'),\n",
       " (1, 'curious'),\n",
       " (1, 'custom'),\n",
       " (3, 'data'),\n",
       " (4, 'default'),\n",
       " (1, 'deliver'),\n",
       " (1, 'dependencies'),\n",
       " (1, 'deploy'),\n",
       " (1, 'described'),\n",
       " (1, 'designed'),\n",
       " (1, 'desktop.'),\n",
       " (1, 'development'),\n",
       " (1, 'die'),\n",
       " (1, 'difficult'),\n",
       " (3, 'distributed'),\n",
       " (3, 'do'),\n",
       " (15, 'docker'),\n",
       " (3, 'docker-compose'),\n",
       " (1, 'docker-compose.'),\n",
       " (1, 'docker-spark-cluster_spark-network'),\n",
       " (1, \"don't\"),\n",
       " (1, 'done'),\n",
       " (3, 'driver'),\n",
       " (2, 'driver-20180923151753-0000'),\n",
       " (1, 'driver-20180923151753-0000\",'),\n",
       " (1, 'driver-20180923151753-0000.'),\n",
       " (1, 'dummy'),\n",
       " (4, 'each'),\n",
       " (1, 'easier'),\n",
       " (1, 'edit'),\n",
       " (2, 'enough'),\n",
       " (1, 'env/spark-worker.sh'),\n",
       " (3, 'environment'),\n",
       " (1, 'environment.'),\n",
       " (6, 'exec'),\n",
       " (1, 'executions'),\n",
       " (1, 'executors'),\n",
       " (3, 'file'),\n",
       " (1, 'file.'),\n",
       " (1, 'file:'),\n",
       " (2, 'files'),\n",
       " (1, 'files.'),\n",
       " (1, 'final'),\n",
       " (2, 'first'),\n",
       " (3, 'folder'),\n",
       " (6, 'following'),\n",
       " (10, 'for'),\n",
       " (2, 'from'),\n",
       " (1, 'gracefully).'),\n",
       " (1, 'guess'),\n",
       " (1, 'had'),\n",
       " (1, 'hand.'),\n",
       " (1, 'has'),\n",
       " (5, 'have'),\n",
       " (1, 'home(just'),\n",
       " (1, 'hot'),\n",
       " (1, 'http://10.5.0.2:8080/'),\n",
       " (1, 'http://10.5.0.3:8081/'),\n",
       " (1, 'http://10.5.0.4:8081/'),\n",
       " (1, 'http://10.5.0.5:8081/'),\n",
       " (2, 'if'),\n",
       " (9, 'image'),\n",
       " (3, 'image,'),\n",
       " (1, 'image.'),\n",
       " (2, 'images'),\n",
       " (1, 'images,'),\n",
       " (1, 'images:'),\n",
       " (4, 'in'),\n",
       " (2, 'input'),\n",
       " (2, 'installed'),\n",
       " (1, 'intended'),\n",
       " (5, 'into'),\n",
       " (14, 'is'),\n",
       " (1, 'it'),\n",
       " (4, 'jar'),\n",
       " (1, 'jars'),\n",
       " (1, 'java:alpine-jdk-8'),\n",
       " (5, 'just'),\n",
       " (1, 'laptop'),\n",
       " (1, 'launch'),\n",
       " (1, 'lazy'),\n",
       " (1, 'let`s'),\n",
       " (1, 'like'),\n",
       " (6, 'ls'),\n",
       " (1, 'main'),\n",
       " (10, 'make'),\n",
       " (8, 'master'),\n",
       " (1, 'master,'),\n",
       " (1, 'modify'),\n",
       " (1, 'more'),\n",
       " (1, 'mounts'),\n",
       " (1, 'much'),\n",
       " (1, 'my'),\n",
       " (1, 'nature'),\n",
       " (1, 'necesary'),\n",
       " (3, 'necessary'),\n",
       " (2, 'need'),\n",
       " (1, 'need.'),\n",
       " (1, 'network'),\n",
       " (1, 'new'),\n",
       " (1, 'node'),\n",
       " (1, 'nodes'),\n",
       " (1, 'not'),\n",
       " (1, 'not...)'),\n",
       " (2, 'now'),\n",
       " (9, 'of'),\n",
       " (10, 'on'),\n",
       " (3, 'one'),\n",
       " (4, 'or'),\n",
       " (1, 'our'),\n",
       " (1, 'output'),\n",
       " (1, 'own'),\n",
       " (1, 'particular'),\n",
       " (1, 'performed'),\n",
       " (1, 'pipelines'),\n",
       " (1, 'place'),\n",
       " (1, 'play'),\n",
       " (1, 'pretty'),\n",
       " (3, 'previously'),\n",
       " (1, 'processed'),\n",
       " (1, 'protocol.'),\n",
       " (1, 'purposses,'),\n",
       " (1, 'purposses.'),\n",
       " (1, 'pyspark'),\n",
       " (1, 'python3'),\n",
       " (1, 'ran'),\n",
       " (1, 'readable'),\n",
       " (1, 'really'),\n",
       " (2, 'request'),\n",
       " (1, 'requisites'),\n",
       " (1, 'resolves'),\n",
       " (1, 'resource'),\n",
       " (2, 'resources'),\n",
       " (1, 'responded'),\n",
       " (7, 'run'),\n",
       " (6, 'running'),\n",
       " (1, 'same'),\n",
       " (2, 'sample'),\n",
       " (2, 'scala'),\n",
       " (1, 'scala,'),\n",
       " (1, 'script.'),\n",
       " (2, 'see'),\n",
       " (1, 'set'),\n",
       " (1, 'ship'),\n",
       " (2, 'shipped'),\n",
       " (1, 'ships'),\n",
       " (2, 'simple'),\n",
       " (1, 'so).'),\n",
       " (1, 'so,'),\n",
       " (1, 'so..).'),\n",
       " (1, 'solution'),\n",
       " (1, 'some'),\n",
       " (27, 'spark'),\n",
       " (1, 'spark-base:2.3.1:'),\n",
       " (1, 'spark-master:2.3.1:'),\n",
       " (1, 'spark-master|10.5.0.2'),\n",
       " (5, 'spark-submit'),\n",
       " (1, 'spark-submit.'),\n",
       " (1, 'spark-submit:2.3.1'),\n",
       " (1, 'spark-submit:2.3.1:'),\n",
       " (1, 'spark-worker'),\n",
       " (2, 'spark-worker-1'),\n",
       " (1, 'spark-worker-1|10.5.0.3'),\n",
       " (2, 'spark-worker-2'),\n",
       " (1, 'spark-worker-2|10.5.0.4'),\n",
       " (2, 'spark-worker-3'),\n",
       " (1, 'spark-worker-3|10.5.0.5'),\n",
       " (1, 'spark-worker:2.3.1:'),\n",
       " (1, 'spark-worker=3'),\n",
       " (1,\n",
       "  'spark.executor.extraJavaOptions=\\'-Dconfig-path=/opt/spark-apps/dev/config.conf\\'\"'),\n",
       " (2, 'spark://spark-master:6066.'),\n",
       " (1, 'spark://spark-master:7077)'),\n",
       " (3, 'standalone'),\n",
       " (1, 'state...'),\n",
       " (1, 'status'),\n",
       " (3, 'step'),\n",
       " (1, 'step,'),\n",
       " (1, 'steps'),\n",
       " (2, 'steps:'),\n",
       " (3, 'submission'),\n",
       " (2, 'submit'),\n",
       " (1, 'submit**'),\n",
       " (1, 'submitted'),\n",
       " (1, 'successful'),\n",
       " (2, 'successfully'),\n",
       " (1, 'support'),\n",
       " (2, 'test'),\n",
       " (1, 'testing'),\n",
       " (1, 'text](docs/spark-master.png'),\n",
       " (1, 'text](docs/spark-worker-1.png'),\n",
       " (1, 'text](docs/spark-worker-2.png'),\n",
       " (1, 'text](docs/spark-worker-3.png'),\n",
       " (34, 'the'),\n",
       " (3, 'these'),\n",
       " (1, 'thing'),\n",
       " (4, 'this'),\n",
       " (1, 'this:'),\n",
       " (1, 'three'),\n",
       " (31, 'to'),\n",
       " (1, 'topic)'),\n",
       " (1, 'toy'),\n",
       " (1, 'true'),\n",
       " (1, 'two'),\n",
       " (1, 'up'),\n",
       " (1, 'up*'),\n",
       " (1, 'us'),\n",
       " (3, 'use'),\n",
       " (8, 'used'),\n",
       " (1, 'useful'),\n",
       " (6, 'using'),\n",
       " (2, 'validate'),\n",
       " (1, 'variables'),\n",
       " (1, 'volume'),\n",
       " (1, 'volumes'),\n",
       " (1, 'was'),\n",
       " (1, 'way'),\n",
       " (1, 'we'),\n",
       " (1, 'wich'),\n",
       " (8, 'will'),\n",
       " (1, 'wish'),\n",
       " (4, 'with'),\n",
       " (1, 'with(Optional)'),\n",
       " (9, 'worker'),\n",
       " (1, 'worker-20180923151711-10.5.0.4-45381'),\n",
       " (3, 'workers'),\n",
       " (3, \"workers's\"),\n",
       " (1, 'workers,'),\n",
       " (10, 'you'),\n",
       " (17, 'your'),\n",
       " (1, '{'),\n",
       " (1, '}')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '\"2.3.1\",'),\n",
       " (1, '\"CreateSubmissionResponse\",'),\n",
       " (1, '\"Driver'),\n",
       " (1, '\"action\"'),\n",
       " (1, '\"driver-20180923151753-0000\",'),\n",
       " (1, '\"message\"'),\n",
       " (1, '\"serverSparkVersion\"'),\n",
       " (1, '\"submissionId\"'),\n",
       " (1, '\"success\"'),\n",
       " (1, '#Creating'),\n",
       " (1, '#Extra'),\n",
       " (1, '#We'),\n",
       " (1, '&&'),\n",
       " (1, \"'ve\"),\n",
       " (1, '(Optional)'),\n",
       " (1, '(What'),\n",
       " (1, '(soon'),\n",
       " (1, '**wild'),\n",
       " (1, '*build-images.sh*'),\n",
       " (1, '*docker-compose'),\n",
       " (1, '+x'),\n",
       " (1, '---|---'),\n",
       " (1, '---|---|---'),\n",
       " (1, '--network'),\n",
       " (1, '--scale'),\n",
       " (1, '-r'),\n",
       " (1, '-v'),\n",
       " (1, './build-images.sh'),\n",
       " (1, '/home/Crimes_-_2001_to_present.csv'),\n",
       " (1, '/home/workspace/crimes-app/build/libs/crimes-app.jar'),\n",
       " (1, '/home/workspace/crimes-app/config'),\n",
       " (1, '/mnt/spark-apps,'),\n",
       " (1, '/mnt/spark-apps:/opt/spark-apps'),\n",
       " (1, '/mnt/spark-apps|/opt/spark-apps|Used'),\n",
       " (1, '/mnt/spark-data|/opt/spark-data|'),\n",
       " (1, '/mnt/spark-files'),\n",
       " (1, '/mnt/spark-files.'),\n",
       " (1, '10.5.0.4:45381.'),\n",
       " (1, '1024'),\n",
       " (1, '128mb'),\n",
       " (1, '15:17:52'),\n",
       " (1, '2.3.1'),\n",
       " (1, '256mb.'),\n",
       " (1, ':(.'),\n",
       " (1, ':O?)'),\n",
       " (1, 'Allocation'),\n",
       " (1, 'Application'),\n",
       " (1, 'Binded'),\n",
       " (1, 'Build'),\n",
       " (1, 'CI/CD'),\n",
       " (1, 'CPU'),\n",
       " (1, 'Check'),\n",
       " (1, 'Cluster'),\n",
       " (1, 'Copied'),\n",
       " (1, 'Copy'),\n",
       " (1, 'Create'),\n",
       " (1, 'CreateSubmissionResponse:'),\n",
       " (1, 'DFS'),\n",
       " (1, 'Driver'),\n",
       " (1, 'General'),\n",
       " (1, 'Host'),\n",
       " (1, \"I've\"),\n",
       " (1, 'If'),\n",
       " (1, 'In'),\n",
       " (1, 'Installation'),\n",
       " (1, 'Jar'),\n",
       " (1, 'Just'),\n",
       " (1, 'Kubernetes'),\n",
       " (1, 'Luckily'),\n",
       " (1, 'MB.'),\n",
       " (1, 'Mesos'),\n",
       " (1, 'Mount|Container'),\n",
       " (1, 'Mount|Purposse'),\n",
       " (1, 'Now'),\n",
       " (1, 'Our'),\n",
       " (1, 'Polling'),\n",
       " (1, 'Pre'),\n",
       " (1, 'REST'),\n",
       " (1, 'RUNNING.'),\n",
       " (1, 'Resource'),\n",
       " (1, 'Right'),\n",
       " (1, 'Running'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=\"/opt/spark-apps/crimes-app.jar\"'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=$SPARK_APPLICATION_JAR_LOCATION'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=\"org.mvb.applications.CrimesApp\"'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=$SPARK_APPLICATION_MAIN_CLASS'),\n",
       " (1, 'SPARK_SUBMIT_ARGS=\"--conf'),\n",
       " (1, 'Scala'),\n",
       " (1, 'Server'),\n",
       " (1, 'Ship'),\n",
       " (1, 'State'),\n",
       " (1, 'Submission'),\n",
       " (1, 'Submitted'),\n",
       " (1, 'Summary'),\n",
       " (1, 'To'),\n",
       " (1, 'UI'),\n",
       " (1, 'URL.'),\n",
       " (1, 'Use'),\n",
       " (1, 'Used'),\n",
       " (1, 'Validate'),\n",
       " (1, 'Volumes'),\n",
       " (1, 'Volumes...(maybe'),\n",
       " (1, 'Why'),\n",
       " (1, 'Workers'),\n",
       " (1, 'Yarn,'),\n",
       " (1, 'You'),\n",
       " (1, '[crimes-app](https://).'),\n",
       " (1, 'accesing'),\n",
       " (1, 'address'),\n",
       " (1, 'allocation(basically'),\n",
       " (1, 'allocation).'),\n",
       " (1, 'allocations'),\n",
       " (1, 'also'),\n",
       " (1, 'am'),\n",
       " (1, 'any'),\n",
       " (1, 'app,'),\n",
       " (1, 'apps'),\n",
       " (1, 'apps(A'),\n",
       " (1, 'args'),\n",
       " (1, 'away'),\n",
       " (1, 'base'),\n",
       " (1, 'because'),\n",
       " (1, 'before'),\n",
       " (1, 'build'),\n",
       " (1, 'build-images.sh'),\n",
       " (1, 'builds'),\n",
       " (1, 'bundle'),\n",
       " (1, 'called'),\n",
       " (1, 'case'),\n",
       " (1, 'chart:'),\n",
       " (1, 'check'),\n",
       " (1, 'chmod'),\n",
       " (1, 'class'),\n",
       " (1, \"cluster's\"),\n",
       " (1, 'cluster(internally'),\n",
       " (1, 'cluster?'),\n",
       " (1, 'command'),\n",
       " (1, 'commands'),\n",
       " (1, 'compiled'),\n",
       " (1, 'configuration'),\n",
       " (1, 'containers(run,'),\n",
       " (1, 'containers:'),\n",
       " (1, 'container|Ip'),\n",
       " (1, 'core.'),\n",
       " (1, 'curious'),\n",
       " (1, 'custom'),\n",
       " (1, 'deliver'),\n",
       " (1, 'dependencies'),\n",
       " (1, 'deploy'),\n",
       " (1, 'described'),\n",
       " (1, 'designed'),\n",
       " (1, 'desktop.'),\n",
       " (1, 'development'),\n",
       " (1, 'die'),\n",
       " (1, 'difficult'),\n",
       " (1, 'docker-compose.'),\n",
       " (1, 'docker-spark-cluster_spark-network'),\n",
       " (1, \"don't\"),\n",
       " (1, 'done'),\n",
       " (1, 'driver-20180923151753-0000\",'),\n",
       " (1, 'driver-20180923151753-0000.'),\n",
       " (1, 'dummy'),\n",
       " (1, 'easier'),\n",
       " (1, 'edit'),\n",
       " (1, 'env/spark-worker.sh'),\n",
       " (1, 'environment.'),\n",
       " (1, 'executions'),\n",
       " (1, 'executors'),\n",
       " (1, 'file.'),\n",
       " (1, 'file:'),\n",
       " (1, 'files.'),\n",
       " (1, 'final'),\n",
       " (1, 'gracefully).'),\n",
       " (1, 'guess'),\n",
       " (1, 'had'),\n",
       " (1, 'hand.'),\n",
       " (1, 'has'),\n",
       " (1, 'home(just'),\n",
       " (1, 'hot'),\n",
       " (1, 'http://10.5.0.2:8080/'),\n",
       " (1, 'http://10.5.0.3:8081/'),\n",
       " (1, 'http://10.5.0.4:8081/'),\n",
       " (1, 'http://10.5.0.5:8081/'),\n",
       " (1, 'image.'),\n",
       " (1, 'images,'),\n",
       " (1, 'images:'),\n",
       " (1, 'intended'),\n",
       " (1, 'it'),\n",
       " (1, 'jars'),\n",
       " (1, 'java:alpine-jdk-8'),\n",
       " (1, 'laptop'),\n",
       " (1, 'launch'),\n",
       " (1, 'lazy'),\n",
       " (1, 'let`s'),\n",
       " (1, 'like'),\n",
       " (1, 'main'),\n",
       " (1, 'master,'),\n",
       " (1, 'modify'),\n",
       " (1, 'more'),\n",
       " (1, 'mounts'),\n",
       " (1, 'much'),\n",
       " (1, 'my'),\n",
       " (1, 'nature'),\n",
       " (1, 'necesary'),\n",
       " (1, 'need.'),\n",
       " (1, 'network'),\n",
       " (1, 'new'),\n",
       " (1, 'node'),\n",
       " (1, 'nodes'),\n",
       " (1, 'not'),\n",
       " (1, 'not...)'),\n",
       " (1, 'our'),\n",
       " (1, 'output'),\n",
       " (1, 'own'),\n",
       " (1, 'particular'),\n",
       " (1, 'performed'),\n",
       " (1, 'pipelines'),\n",
       " (1, 'place'),\n",
       " (1, 'play'),\n",
       " (1, 'pretty'),\n",
       " (1, 'processed'),\n",
       " (1, 'protocol.'),\n",
       " (1, 'purposses,'),\n",
       " (1, 'purposses.'),\n",
       " (1, 'pyspark'),\n",
       " (1, 'python3'),\n",
       " (1, 'ran'),\n",
       " (1, 'readable'),\n",
       " (1, 'really'),\n",
       " (1, 'requisites'),\n",
       " (1, 'resolves'),\n",
       " (1, 'resource'),\n",
       " (1, 'responded'),\n",
       " (1, 'same'),\n",
       " (1, 'scala,'),\n",
       " (1, 'script.'),\n",
       " (1, 'set'),\n",
       " (1, 'ship'),\n",
       " (1, 'ships'),\n",
       " (1, 'so).'),\n",
       " (1, 'so,'),\n",
       " (1, 'so..).'),\n",
       " (1, 'solution'),\n",
       " (1, 'some'),\n",
       " (1, 'spark-base:2.3.1:'),\n",
       " (1, 'spark-master:2.3.1:'),\n",
       " (1, 'spark-master|10.5.0.2'),\n",
       " (1, 'spark-submit.'),\n",
       " (1, 'spark-submit:2.3.1'),\n",
       " (1, 'spark-submit:2.3.1:'),\n",
       " (1, 'spark-worker'),\n",
       " (1, 'spark-worker-1|10.5.0.3'),\n",
       " (1, 'spark-worker-2|10.5.0.4'),\n",
       " (1, 'spark-worker-3|10.5.0.5'),\n",
       " (1, 'spark-worker:2.3.1:'),\n",
       " (1, 'spark-worker=3'),\n",
       " (1,\n",
       "  'spark.executor.extraJavaOptions=\\'-Dconfig-path=/opt/spark-apps/dev/config.conf\\'\"'),\n",
       " (1, 'spark://spark-master:7077)'),\n",
       " (1, 'state...'),\n",
       " (1, 'status'),\n",
       " (1, 'step,'),\n",
       " (1, 'steps'),\n",
       " (1, 'submit**'),\n",
       " (1, 'submitted'),\n",
       " (1, 'successful'),\n",
       " (1, 'support'),\n",
       " (1, 'testing'),\n",
       " (1, 'text](docs/spark-master.png'),\n",
       " (1, 'text](docs/spark-worker-1.png'),\n",
       " (1, 'text](docs/spark-worker-2.png'),\n",
       " (1, 'text](docs/spark-worker-3.png'),\n",
       " (1, 'thing'),\n",
       " (1, 'this:'),\n",
       " (1, 'three'),\n",
       " (1, 'topic)'),\n",
       " (1, 'toy'),\n",
       " (1, 'true'),\n",
       " (1, 'two'),\n",
       " (1, 'up'),\n",
       " (1, 'up*'),\n",
       " (1, 'us'),\n",
       " (1, 'useful'),\n",
       " (1, 'variables'),\n",
       " (1, 'volume'),\n",
       " (1, 'volumes'),\n",
       " (1, 'was'),\n",
       " (1, 'way'),\n",
       " (1, 'we'),\n",
       " (1, 'wich'),\n",
       " (1, 'wish'),\n",
       " (1, 'with(Optional)'),\n",
       " (1, 'worker-20180923151711-10.5.0.4-45381'),\n",
       " (1, 'workers,'),\n",
       " (1, '{'),\n",
       " (1, '}'),\n",
       " (2, '#App'),\n",
       " (2, '#Copy'),\n",
       " (2, '**spark-submit**'),\n",
       " (2, '--env'),\n",
       " (2, '/mnt/spark-apps'),\n",
       " (2, 'After'),\n",
       " (2, 'Master'),\n",
       " (2, 'Run'),\n",
       " (2, 'Submitting'),\n",
       " (2, 'application.'),\n",
       " (2, 'available'),\n",
       " (2, 'basically'),\n",
       " (2, 'code'),\n",
       " (2, 'configs'),\n",
       " (2, 'cpu'),\n",
       " (2, 'driver-20180923151753-0000'),\n",
       " (2, 'enough'),\n",
       " (2, 'files'),\n",
       " (2, 'first'),\n",
       " (2, 'from'),\n",
       " (2, 'if'),\n",
       " (2, 'images'),\n",
       " (2, 'input'),\n",
       " (2, 'installed'),\n",
       " (2, 'need'),\n",
       " (2, 'now'),\n",
       " (2, 'request'),\n",
       " (2, 'resources'),\n",
       " (2, 'sample'),\n",
       " (2, 'scala'),\n",
       " (2, 'see'),\n",
       " (2, 'shipped'),\n",
       " (2, 'simple'),\n",
       " (2, 'spark-worker-1'),\n",
       " (2, 'spark-worker-2'),\n",
       " (2, 'spark-worker-3'),\n",
       " (2, 'spark://spark-master:6066.'),\n",
       " (2, 'steps:'),\n",
       " (2, 'submit'),\n",
       " (2, 'successfully'),\n",
       " (2, 'test'),\n",
       " (2, 'validate'),\n",
       " (3, '/opt/spark-apps'),\n",
       " (3, '/opt/spark-data'),\n",
       " (3, '2'),\n",
       " (3, 'Validations'),\n",
       " (3, 'We'),\n",
       " (3, '```bash'),\n",
       " (3, '```sh'),\n",
       " (3, 'allocation'),\n",
       " (3, \"app's\"),\n",
       " (3, 'are'),\n",
       " (3, 'at'),\n",
       " (3, 'by'),\n",
       " (3, 'can'),\n",
       " (3, 'compose'),\n",
       " (3, 'copy'),\n",
       " (3, 'cores'),\n",
       " (3, 'cp'),\n",
       " (3, 'data'),\n",
       " (3, 'distributed'),\n",
       " (3, 'do'),\n",
       " (3, 'docker-compose'),\n",
       " (3, 'driver'),\n",
       " (3, 'environment'),\n",
       " (3, 'file'),\n",
       " (3, 'folder'),\n",
       " (3, 'image,'),\n",
       " (3, 'necessary'),\n",
       " (3, 'one'),\n",
       " (3, 'previously'),\n",
       " (3, 'standalone'),\n",
       " (3, 'step'),\n",
       " (3, 'submission'),\n",
       " (3, 'these'),\n",
       " (3, 'use'),\n",
       " (3, 'workers'),\n",
       " (3, \"workers's\"),\n",
       " (4, '![alt'),\n",
       " (4, '\"Spark'),\n",
       " (4, '###'),\n",
       " (4, '3'),\n",
       " (4, 'Docker'),\n",
       " (4, 'UI\")'),\n",
       " (4, '\\\\'),\n",
       " (4, 'an'),\n",
       " (4, 'containers.'),\n",
       " (4, 'default'),\n",
       " (4, 'each'),\n",
       " (4, 'in'),\n",
       " (4, 'jar'),\n",
       " (4, 'or'),\n",
       " (4, 'this'),\n",
       " (4, 'with'),\n",
       " (5, '1'),\n",
       " (5, '15:17:53'),\n",
       " (5, ':'),\n",
       " (5, 'RAM'),\n",
       " (5, 'based'),\n",
       " (5, 'have'),\n",
       " (5, 'into'),\n",
       " (5, 'just'),\n",
       " (5, 'spark-submit'),\n",
       " (6, '&'),\n",
       " (6, '-'),\n",
       " (6, '-l'),\n",
       " (6, '-ti'),\n",
       " (6, '2018-09-23'),\n",
       " (6, 'I'),\n",
       " (6, 'INFO'),\n",
       " (6, 'RestSubmissionClient:54'),\n",
       " (6, 'Spark'),\n",
       " (6, 'This'),\n",
       " (6, 'Worker'),\n",
       " (6, '```'),\n",
       " (6, 'all'),\n",
       " (6, 'as'),\n",
       " (6, 'be'),\n",
       " (6, 'create'),\n",
       " (6, 'created'),\n",
       " (6, 'exec'),\n",
       " (6, 'following'),\n",
       " (6, 'ls'),\n",
       " (6, 'running'),\n",
       " (6, 'using'),\n",
       " (7, 'run'),\n",
       " (8, '##'),\n",
       " (8, 'A'),\n",
       " (8, 'app'),\n",
       " (8, 'application'),\n",
       " (8, 'master'),\n",
       " (8, 'used'),\n",
       " (8, 'will'),\n",
       " (9, 'cluster'),\n",
       " (9, 'image'),\n",
       " (9, 'of'),\n",
       " (9, 'worker'),\n",
       " (10, 'The'),\n",
       " (10, 'for'),\n",
       " (10, 'make'),\n",
       " (10, 'on'),\n",
       " (10, 'you'),\n",
       " (12, '#'),\n",
       " (13, 'and'),\n",
       " (14, 'is'),\n",
       " (15, 'docker'),\n",
       " (17, 'your'),\n",
       " (18, 'a'),\n",
       " (20, '*'),\n",
       " (27, 'spark'),\n",
       " (31, 'to'),\n",
       " (34, 'the'),\n",
       " (104, '')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(104, ''),\n",
       " (4, '![alt'),\n",
       " (1, '\"2.3.1\",'),\n",
       " (1, '\"CreateSubmissionResponse\",'),\n",
       " (1, '\"Driver'),\n",
       " (4, '\"Spark'),\n",
       " (1, '\"action\"'),\n",
       " (1, '\"driver-20180923151753-0000\",'),\n",
       " (1, '\"message\"'),\n",
       " (1, '\"serverSparkVersion\"'),\n",
       " (1, '\"submissionId\"'),\n",
       " (1, '\"success\"'),\n",
       " (12, '#'),\n",
       " (8, '##'),\n",
       " (4, '###'),\n",
       " (2, '#App'),\n",
       " (2, '#Copy'),\n",
       " (1, '#Creating'),\n",
       " (1, '#Extra'),\n",
       " (1, '#We'),\n",
       " (6, '&'),\n",
       " (1, '&&'),\n",
       " (1, \"'ve\"),\n",
       " (1, '(Optional)'),\n",
       " (1, '(What'),\n",
       " (1, '(soon'),\n",
       " (20, '*'),\n",
       " (2, '**spark-submit**'),\n",
       " (1, '**wild'),\n",
       " (1, '*build-images.sh*'),\n",
       " (1, '*docker-compose'),\n",
       " (1, '+x'),\n",
       " (6, '-'),\n",
       " (1, '---|---'),\n",
       " (1, '---|---|---'),\n",
       " (2, '--env'),\n",
       " (1, '--network'),\n",
       " (1, '--scale'),\n",
       " (6, '-l'),\n",
       " (1, '-r'),\n",
       " (6, '-ti'),\n",
       " (1, '-v'),\n",
       " (1, './build-images.sh'),\n",
       " (1, '/home/Crimes_-_2001_to_present.csv'),\n",
       " (1, '/home/workspace/crimes-app/build/libs/crimes-app.jar'),\n",
       " (1, '/home/workspace/crimes-app/config'),\n",
       " (2, '/mnt/spark-apps'),\n",
       " (1, '/mnt/spark-apps,'),\n",
       " (1, '/mnt/spark-apps:/opt/spark-apps'),\n",
       " (1, '/mnt/spark-apps|/opt/spark-apps|Used'),\n",
       " (1, '/mnt/spark-data|/opt/spark-data|'),\n",
       " (1, '/mnt/spark-files'),\n",
       " (1, '/mnt/spark-files.'),\n",
       " (3, '/opt/spark-apps'),\n",
       " (3, '/opt/spark-data'),\n",
       " (5, '1'),\n",
       " (1, '10.5.0.4:45381.'),\n",
       " (1, '1024'),\n",
       " (1, '128mb'),\n",
       " (1, '15:17:52'),\n",
       " (5, '15:17:53'),\n",
       " (3, '2'),\n",
       " (1, '2.3.1'),\n",
       " (6, '2018-09-23'),\n",
       " (1, '256mb.'),\n",
       " (4, '3'),\n",
       " (5, ':'),\n",
       " (1, ':(.'),\n",
       " (1, ':O?)'),\n",
       " (8, 'A'),\n",
       " (2, 'After'),\n",
       " (1, 'Allocation'),\n",
       " (1, 'Application'),\n",
       " (1, 'Binded'),\n",
       " (1, 'Build'),\n",
       " (1, 'CI/CD'),\n",
       " (1, 'CPU'),\n",
       " (1, 'Check'),\n",
       " (1, 'Cluster'),\n",
       " (1, 'Copied'),\n",
       " (1, 'Copy'),\n",
       " (1, 'Create'),\n",
       " (1, 'CreateSubmissionResponse:'),\n",
       " (1, 'DFS'),\n",
       " (4, 'Docker'),\n",
       " (1, 'Driver'),\n",
       " (1, 'General'),\n",
       " (1, 'Host'),\n",
       " (6, 'I'),\n",
       " (1, \"I've\"),\n",
       " (6, 'INFO'),\n",
       " (1, 'If'),\n",
       " (1, 'In'),\n",
       " (1, 'Installation'),\n",
       " (1, 'Jar'),\n",
       " (1, 'Just'),\n",
       " (1, 'Kubernetes'),\n",
       " (1, 'Luckily'),\n",
       " (1, 'MB.'),\n",
       " (2, 'Master'),\n",
       " (1, 'Mesos'),\n",
       " (1, 'Mount|Container'),\n",
       " (1, 'Mount|Purposse'),\n",
       " (1, 'Now'),\n",
       " (1, 'Our'),\n",
       " (1, 'Polling'),\n",
       " (1, 'Pre'),\n",
       " (5, 'RAM'),\n",
       " (1, 'REST'),\n",
       " (1, 'RUNNING.'),\n",
       " (1, 'Resource'),\n",
       " (6, 'RestSubmissionClient:54'),\n",
       " (1, 'Right'),\n",
       " (2, 'Run'),\n",
       " (1, 'Running'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=\"/opt/spark-apps/crimes-app.jar\"'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=$SPARK_APPLICATION_JAR_LOCATION'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=\"org.mvb.applications.CrimesApp\"'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=$SPARK_APPLICATION_MAIN_CLASS'),\n",
       " (1, 'SPARK_SUBMIT_ARGS=\"--conf'),\n",
       " (1, 'Scala'),\n",
       " (1, 'Server'),\n",
       " (1, 'Ship'),\n",
       " (6, 'Spark'),\n",
       " (1, 'State'),\n",
       " (1, 'Submission'),\n",
       " (1, 'Submitted'),\n",
       " (2, 'Submitting'),\n",
       " (1, 'Summary'),\n",
       " (10, 'The'),\n",
       " (6, 'This'),\n",
       " (1, 'To'),\n",
       " (1, 'UI'),\n",
       " (4, 'UI\")'),\n",
       " (1, 'URL.'),\n",
       " (1, 'Use'),\n",
       " (1, 'Used'),\n",
       " (1, 'Validate'),\n",
       " (3, 'Validations'),\n",
       " (1, 'Volumes'),\n",
       " (1, 'Volumes...(maybe'),\n",
       " (3, 'We'),\n",
       " (1, 'Why'),\n",
       " (6, 'Worker'),\n",
       " (1, 'Workers'),\n",
       " (1, 'Yarn,'),\n",
       " (1, 'You'),\n",
       " (1, '[crimes-app](https://).'),\n",
       " (4, '\\\\'),\n",
       " (6, '```'),\n",
       " (3, '```bash'),\n",
       " (3, '```sh'),\n",
       " (18, 'a'),\n",
       " (1, 'accesing'),\n",
       " (1, 'address'),\n",
       " (6, 'all'),\n",
       " (3, 'allocation'),\n",
       " (1, 'allocation(basically'),\n",
       " (1, 'allocation).'),\n",
       " (1, 'allocations'),\n",
       " (1, 'also'),\n",
       " (1, 'am'),\n",
       " (4, 'an'),\n",
       " (13, 'and'),\n",
       " (1, 'any'),\n",
       " (8, 'app'),\n",
       " (3, \"app's\"),\n",
       " (1, 'app,'),\n",
       " (8, 'application'),\n",
       " (2, 'application.'),\n",
       " (1, 'apps'),\n",
       " (1, 'apps(A'),\n",
       " (3, 'are'),\n",
       " (1, 'args'),\n",
       " (6, 'as'),\n",
       " (3, 'at'),\n",
       " (2, 'available'),\n",
       " (1, 'away'),\n",
       " (1, 'base'),\n",
       " (5, 'based'),\n",
       " (2, 'basically'),\n",
       " (6, 'be'),\n",
       " (1, 'because'),\n",
       " (1, 'before'),\n",
       " (1, 'build'),\n",
       " (1, 'build-images.sh'),\n",
       " (1, 'builds'),\n",
       " (1, 'bundle'),\n",
       " (3, 'by'),\n",
       " (1, 'called'),\n",
       " (3, 'can'),\n",
       " (1, 'case'),\n",
       " (1, 'chart:'),\n",
       " (1, 'check'),\n",
       " (1, 'chmod'),\n",
       " (1, 'class'),\n",
       " (9, 'cluster'),\n",
       " (1, \"cluster's\"),\n",
       " (1, 'cluster(internally'),\n",
       " (1, 'cluster?'),\n",
       " (2, 'code'),\n",
       " (1, 'command'),\n",
       " (1, 'commands'),\n",
       " (1, 'compiled'),\n",
       " (3, 'compose'),\n",
       " (2, 'configs'),\n",
       " (1, 'configuration'),\n",
       " (1, 'containers(run,'),\n",
       " (4, 'containers.'),\n",
       " (1, 'containers:'),\n",
       " (1, 'container|Ip'),\n",
       " (3, 'copy'),\n",
       " (1, 'core.'),\n",
       " (3, 'cores'),\n",
       " (3, 'cp'),\n",
       " (2, 'cpu'),\n",
       " (6, 'create'),\n",
       " (6, 'created'),\n",
       " (1, 'curious'),\n",
       " (1, 'custom'),\n",
       " (3, 'data'),\n",
       " (4, 'default'),\n",
       " (1, 'deliver'),\n",
       " (1, 'dependencies'),\n",
       " (1, 'deploy'),\n",
       " (1, 'described'),\n",
       " (1, 'designed'),\n",
       " (1, 'desktop.'),\n",
       " (1, 'development'),\n",
       " (1, 'die'),\n",
       " (1, 'difficult'),\n",
       " (3, 'distributed'),\n",
       " (3, 'do'),\n",
       " (15, 'docker'),\n",
       " (3, 'docker-compose'),\n",
       " (1, 'docker-compose.'),\n",
       " (1, 'docker-spark-cluster_spark-network'),\n",
       " (1, \"don't\"),\n",
       " (1, 'done'),\n",
       " (3, 'driver'),\n",
       " (2, 'driver-20180923151753-0000'),\n",
       " (1, 'driver-20180923151753-0000\",'),\n",
       " (1, 'driver-20180923151753-0000.'),\n",
       " (1, 'dummy'),\n",
       " (4, 'each'),\n",
       " (1, 'easier'),\n",
       " (1, 'edit'),\n",
       " (2, 'enough'),\n",
       " (1, 'env/spark-worker.sh'),\n",
       " (3, 'environment'),\n",
       " (1, 'environment.'),\n",
       " (6, 'exec'),\n",
       " (1, 'executions'),\n",
       " (1, 'executors'),\n",
       " (3, 'file'),\n",
       " (1, 'file.'),\n",
       " (1, 'file:'),\n",
       " (2, 'files'),\n",
       " (1, 'files.'),\n",
       " (1, 'final'),\n",
       " (2, 'first'),\n",
       " (3, 'folder'),\n",
       " (6, 'following'),\n",
       " (10, 'for'),\n",
       " (2, 'from'),\n",
       " (1, 'gracefully).'),\n",
       " (1, 'guess'),\n",
       " (1, 'had'),\n",
       " (1, 'hand.'),\n",
       " (1, 'has'),\n",
       " (5, 'have'),\n",
       " (1, 'home(just'),\n",
       " (1, 'hot'),\n",
       " (1, 'http://10.5.0.2:8080/'),\n",
       " (1, 'http://10.5.0.3:8081/'),\n",
       " (1, 'http://10.5.0.4:8081/'),\n",
       " (1, 'http://10.5.0.5:8081/'),\n",
       " (2, 'if'),\n",
       " (9, 'image'),\n",
       " (3, 'image,'),\n",
       " (1, 'image.'),\n",
       " (2, 'images'),\n",
       " (1, 'images,'),\n",
       " (1, 'images:'),\n",
       " (4, 'in'),\n",
       " (2, 'input'),\n",
       " (2, 'installed'),\n",
       " (1, 'intended'),\n",
       " (5, 'into'),\n",
       " (14, 'is'),\n",
       " (1, 'it'),\n",
       " (4, 'jar'),\n",
       " (1, 'jars'),\n",
       " (1, 'java:alpine-jdk-8'),\n",
       " (5, 'just'),\n",
       " (1, 'laptop'),\n",
       " (1, 'launch'),\n",
       " (1, 'lazy'),\n",
       " (1, 'let`s'),\n",
       " (1, 'like'),\n",
       " (6, 'ls'),\n",
       " (1, 'main'),\n",
       " (10, 'make'),\n",
       " (8, 'master'),\n",
       " (1, 'master,'),\n",
       " (1, 'modify'),\n",
       " (1, 'more'),\n",
       " (1, 'mounts'),\n",
       " (1, 'much'),\n",
       " (1, 'my'),\n",
       " (1, 'nature'),\n",
       " (1, 'necesary'),\n",
       " (3, 'necessary'),\n",
       " (2, 'need'),\n",
       " (1, 'need.'),\n",
       " (1, 'network'),\n",
       " (1, 'new'),\n",
       " (1, 'node'),\n",
       " (1, 'nodes'),\n",
       " (1, 'not'),\n",
       " (1, 'not...)'),\n",
       " (2, 'now'),\n",
       " (9, 'of'),\n",
       " (10, 'on'),\n",
       " (3, 'one'),\n",
       " (4, 'or'),\n",
       " (1, 'our'),\n",
       " (1, 'output'),\n",
       " (1, 'own'),\n",
       " (1, 'particular'),\n",
       " (1, 'performed'),\n",
       " (1, 'pipelines'),\n",
       " (1, 'place'),\n",
       " (1, 'play'),\n",
       " (1, 'pretty'),\n",
       " (3, 'previously'),\n",
       " (1, 'processed'),\n",
       " (1, 'protocol.'),\n",
       " (1, 'purposses,'),\n",
       " (1, 'purposses.'),\n",
       " (1, 'pyspark'),\n",
       " (1, 'python3'),\n",
       " (1, 'ran'),\n",
       " (1, 'readable'),\n",
       " (1, 'really'),\n",
       " (2, 'request'),\n",
       " (1, 'requisites'),\n",
       " (1, 'resolves'),\n",
       " (1, 'resource'),\n",
       " (2, 'resources'),\n",
       " (1, 'responded'),\n",
       " (7, 'run'),\n",
       " (6, 'running'),\n",
       " (1, 'same'),\n",
       " (2, 'sample'),\n",
       " (2, 'scala'),\n",
       " (1, 'scala,'),\n",
       " (1, 'script.'),\n",
       " (2, 'see'),\n",
       " (1, 'set'),\n",
       " (1, 'ship'),\n",
       " (2, 'shipped'),\n",
       " (1, 'ships'),\n",
       " (2, 'simple'),\n",
       " (1, 'so).'),\n",
       " (1, 'so,'),\n",
       " (1, 'so..).'),\n",
       " (1, 'solution'),\n",
       " (1, 'some'),\n",
       " (27, 'spark'),\n",
       " (1, 'spark-base:2.3.1:'),\n",
       " (1, 'spark-master:2.3.1:'),\n",
       " (1, 'spark-master|10.5.0.2'),\n",
       " (5, 'spark-submit'),\n",
       " (1, 'spark-submit.'),\n",
       " (1, 'spark-submit:2.3.1'),\n",
       " (1, 'spark-submit:2.3.1:'),\n",
       " (1, 'spark-worker'),\n",
       " (2, 'spark-worker-1'),\n",
       " (1, 'spark-worker-1|10.5.0.3'),\n",
       " (2, 'spark-worker-2'),\n",
       " (1, 'spark-worker-2|10.5.0.4'),\n",
       " (2, 'spark-worker-3'),\n",
       " (1, 'spark-worker-3|10.5.0.5'),\n",
       " (1, 'spark-worker:2.3.1:'),\n",
       " (1, 'spark-worker=3'),\n",
       " (1,\n",
       "  'spark.executor.extraJavaOptions=\\'-Dconfig-path=/opt/spark-apps/dev/config.conf\\'\"'),\n",
       " (2, 'spark://spark-master:6066.'),\n",
       " (1, 'spark://spark-master:7077)'),\n",
       " (3, 'standalone'),\n",
       " (1, 'state...'),\n",
       " (1, 'status'),\n",
       " (3, 'step'),\n",
       " (1, 'step,'),\n",
       " (1, 'steps'),\n",
       " (2, 'steps:'),\n",
       " (3, 'submission'),\n",
       " (2, 'submit'),\n",
       " (1, 'submit**'),\n",
       " (1, 'submitted'),\n",
       " (1, 'successful'),\n",
       " (2, 'successfully'),\n",
       " (1, 'support'),\n",
       " (2, 'test'),\n",
       " (1, 'testing'),\n",
       " (1, 'text](docs/spark-master.png'),\n",
       " (1, 'text](docs/spark-worker-1.png'),\n",
       " (1, 'text](docs/spark-worker-2.png'),\n",
       " (1, 'text](docs/spark-worker-3.png'),\n",
       " (34, 'the'),\n",
       " (3, 'these'),\n",
       " (1, 'thing'),\n",
       " (4, 'this'),\n",
       " (1, 'this:'),\n",
       " (1, 'three'),\n",
       " (31, 'to'),\n",
       " (1, 'topic)'),\n",
       " (1, 'toy'),\n",
       " (1, 'true'),\n",
       " (1, 'two'),\n",
       " (1, 'up'),\n",
       " (1, 'up*'),\n",
       " (1, 'us'),\n",
       " (3, 'use'),\n",
       " (8, 'used'),\n",
       " (1, 'useful'),\n",
       " (6, 'using'),\n",
       " (2, 'validate'),\n",
       " (1, 'variables'),\n",
       " (1, 'volume'),\n",
       " (1, 'volumes'),\n",
       " (1, 'was'),\n",
       " (1, 'way'),\n",
       " (1, 'we'),\n",
       " (1, 'wich'),\n",
       " (8, 'will'),\n",
       " (1, 'wish'),\n",
       " (4, 'with'),\n",
       " (1, 'with(Optional)'),\n",
       " (9, 'worker'),\n",
       " (1, 'worker-20180923151711-10.5.0.4-45381'),\n",
       " (3, 'workers'),\n",
       " (3, \"workers's\"),\n",
       " (1, 'workers,'),\n",
       " (10, 'you'),\n",
       " (17, 'your'),\n",
       " (1, '{'),\n",
       " (1, '}')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#value\n",
    "rdd2.sortBy(ascending=True,numPartitions=None, keyfunc = lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '\"2.3.1\",'),\n",
       " (1, '\"CreateSubmissionResponse\",'),\n",
       " (1, '\"Driver'),\n",
       " (1, '\"action\"'),\n",
       " (1, '\"driver-20180923151753-0000\",'),\n",
       " (1, '\"message\"'),\n",
       " (1, '\"serverSparkVersion\"'),\n",
       " (1, '\"submissionId\"'),\n",
       " (1, '\"success\"'),\n",
       " (1, '#Creating'),\n",
       " (1, '#Extra'),\n",
       " (1, '#We'),\n",
       " (1, '&&'),\n",
       " (1, \"'ve\"),\n",
       " (1, '(Optional)'),\n",
       " (1, '(What'),\n",
       " (1, '(soon'),\n",
       " (1, '**wild'),\n",
       " (1, '*build-images.sh*'),\n",
       " (1, '*docker-compose'),\n",
       " (1, '+x'),\n",
       " (1, '---|---'),\n",
       " (1, '---|---|---'),\n",
       " (1, '--network'),\n",
       " (1, '--scale'),\n",
       " (1, '-r'),\n",
       " (1, '-v'),\n",
       " (1, './build-images.sh'),\n",
       " (1, '/home/Crimes_-_2001_to_present.csv'),\n",
       " (1, '/home/workspace/crimes-app/build/libs/crimes-app.jar'),\n",
       " (1, '/home/workspace/crimes-app/config'),\n",
       " (1, '/mnt/spark-apps,'),\n",
       " (1, '/mnt/spark-apps:/opt/spark-apps'),\n",
       " (1, '/mnt/spark-apps|/opt/spark-apps|Used'),\n",
       " (1, '/mnt/spark-data|/opt/spark-data|'),\n",
       " (1, '/mnt/spark-files'),\n",
       " (1, '/mnt/spark-files.'),\n",
       " (1, '10.5.0.4:45381.'),\n",
       " (1, '1024'),\n",
       " (1, '128mb'),\n",
       " (1, '15:17:52'),\n",
       " (1, '2.3.1'),\n",
       " (1, '256mb.'),\n",
       " (1, ':(.'),\n",
       " (1, ':O?)'),\n",
       " (1, 'Allocation'),\n",
       " (1, 'Application'),\n",
       " (1, 'Binded'),\n",
       " (1, 'Build'),\n",
       " (1, 'CI/CD'),\n",
       " (1, 'CPU'),\n",
       " (1, 'Check'),\n",
       " (1, 'Cluster'),\n",
       " (1, 'Copied'),\n",
       " (1, 'Copy'),\n",
       " (1, 'Create'),\n",
       " (1, 'CreateSubmissionResponse:'),\n",
       " (1, 'DFS'),\n",
       " (1, 'Driver'),\n",
       " (1, 'General'),\n",
       " (1, 'Host'),\n",
       " (1, \"I've\"),\n",
       " (1, 'If'),\n",
       " (1, 'In'),\n",
       " (1, 'Installation'),\n",
       " (1, 'Jar'),\n",
       " (1, 'Just'),\n",
       " (1, 'Kubernetes'),\n",
       " (1, 'Luckily'),\n",
       " (1, 'MB.'),\n",
       " (1, 'Mesos'),\n",
       " (1, 'Mount|Container'),\n",
       " (1, 'Mount|Purposse'),\n",
       " (1, 'Now'),\n",
       " (1, 'Our'),\n",
       " (1, 'Polling'),\n",
       " (1, 'Pre'),\n",
       " (1, 'REST'),\n",
       " (1, 'RUNNING.'),\n",
       " (1, 'Resource'),\n",
       " (1, 'Right'),\n",
       " (1, 'Running'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=\"/opt/spark-apps/crimes-app.jar\"'),\n",
       " (1, 'SPARK_APPLICATION_JAR_LOCATION=$SPARK_APPLICATION_JAR_LOCATION'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=\"org.mvb.applications.CrimesApp\"'),\n",
       " (1, 'SPARK_APPLICATION_MAIN_CLASS=$SPARK_APPLICATION_MAIN_CLASS'),\n",
       " (1, 'SPARK_SUBMIT_ARGS=\"--conf'),\n",
       " (1, 'Scala'),\n",
       " (1, 'Server'),\n",
       " (1, 'Ship'),\n",
       " (1, 'State'),\n",
       " (1, 'Submission'),\n",
       " (1, 'Submitted'),\n",
       " (1, 'Summary'),\n",
       " (1, 'To'),\n",
       " (1, 'UI'),\n",
       " (1, 'URL.'),\n",
       " (1, 'Use'),\n",
       " (1, 'Used'),\n",
       " (1, 'Validate'),\n",
       " (1, 'Volumes'),\n",
       " (1, 'Volumes...(maybe'),\n",
       " (1, 'Why'),\n",
       " (1, 'Workers'),\n",
       " (1, 'Yarn,'),\n",
       " (1, 'You'),\n",
       " (1, '[crimes-app](https://).'),\n",
       " (1, 'accesing'),\n",
       " (1, 'address'),\n",
       " (1, 'allocation(basically'),\n",
       " (1, 'allocation).'),\n",
       " (1, 'allocations'),\n",
       " (1, 'also'),\n",
       " (1, 'am'),\n",
       " (1, 'any'),\n",
       " (1, 'app,'),\n",
       " (1, 'apps'),\n",
       " (1, 'apps(A'),\n",
       " (1, 'args'),\n",
       " (1, 'away'),\n",
       " (1, 'base'),\n",
       " (1, 'because'),\n",
       " (1, 'before'),\n",
       " (1, 'build'),\n",
       " (1, 'build-images.sh'),\n",
       " (1, 'builds'),\n",
       " (1, 'bundle'),\n",
       " (1, 'called'),\n",
       " (1, 'case'),\n",
       " (1, 'chart:'),\n",
       " (1, 'check'),\n",
       " (1, 'chmod'),\n",
       " (1, 'class'),\n",
       " (1, \"cluster's\"),\n",
       " (1, 'cluster(internally'),\n",
       " (1, 'cluster?'),\n",
       " (1, 'command'),\n",
       " (1, 'commands'),\n",
       " (1, 'compiled'),\n",
       " (1, 'configuration'),\n",
       " (1, 'containers(run,'),\n",
       " (1, 'containers:'),\n",
       " (1, 'container|Ip'),\n",
       " (1, 'core.'),\n",
       " (1, 'curious'),\n",
       " (1, 'custom'),\n",
       " (1, 'deliver'),\n",
       " (1, 'dependencies'),\n",
       " (1, 'deploy'),\n",
       " (1, 'described'),\n",
       " (1, 'designed'),\n",
       " (1, 'desktop.'),\n",
       " (1, 'development'),\n",
       " (1, 'die'),\n",
       " (1, 'difficult'),\n",
       " (1, 'docker-compose.'),\n",
       " (1, 'docker-spark-cluster_spark-network'),\n",
       " (1, \"don't\"),\n",
       " (1, 'done'),\n",
       " (1, 'driver-20180923151753-0000\",'),\n",
       " (1, 'driver-20180923151753-0000.'),\n",
       " (1, 'dummy'),\n",
       " (1, 'easier'),\n",
       " (1, 'edit'),\n",
       " (1, 'env/spark-worker.sh'),\n",
       " (1, 'environment.'),\n",
       " (1, 'executions'),\n",
       " (1, 'executors'),\n",
       " (1, 'file.'),\n",
       " (1, 'file:'),\n",
       " (1, 'files.'),\n",
       " (1, 'final'),\n",
       " (1, 'gracefully).'),\n",
       " (1, 'guess'),\n",
       " (1, 'had'),\n",
       " (1, 'hand.'),\n",
       " (1, 'has'),\n",
       " (1, 'home(just'),\n",
       " (1, 'hot'),\n",
       " (1, 'http://10.5.0.2:8080/'),\n",
       " (1, 'http://10.5.0.3:8081/'),\n",
       " (1, 'http://10.5.0.4:8081/'),\n",
       " (1, 'http://10.5.0.5:8081/'),\n",
       " (1, 'image.'),\n",
       " (1, 'images,'),\n",
       " (1, 'images:'),\n",
       " (1, 'intended'),\n",
       " (1, 'it'),\n",
       " (1, 'jars'),\n",
       " (1, 'java:alpine-jdk-8'),\n",
       " (1, 'laptop'),\n",
       " (1, 'launch'),\n",
       " (1, 'lazy'),\n",
       " (1, 'let`s'),\n",
       " (1, 'like'),\n",
       " (1, 'main'),\n",
       " (1, 'master,'),\n",
       " (1, 'modify'),\n",
       " (1, 'more'),\n",
       " (1, 'mounts'),\n",
       " (1, 'much'),\n",
       " (1, 'my'),\n",
       " (1, 'nature'),\n",
       " (1, 'necesary'),\n",
       " (1, 'need.'),\n",
       " (1, 'network'),\n",
       " (1, 'new'),\n",
       " (1, 'node'),\n",
       " (1, 'nodes'),\n",
       " (1, 'not'),\n",
       " (1, 'not...)'),\n",
       " (1, 'our'),\n",
       " (1, 'output'),\n",
       " (1, 'own'),\n",
       " (1, 'particular'),\n",
       " (1, 'performed'),\n",
       " (1, 'pipelines'),\n",
       " (1, 'place'),\n",
       " (1, 'play'),\n",
       " (1, 'pretty'),\n",
       " (1, 'processed'),\n",
       " (1, 'protocol.'),\n",
       " (1, 'purposses,'),\n",
       " (1, 'purposses.'),\n",
       " (1, 'pyspark'),\n",
       " (1, 'python3'),\n",
       " (1, 'ran'),\n",
       " (1, 'readable'),\n",
       " (1, 'really'),\n",
       " (1, 'requisites'),\n",
       " (1, 'resolves'),\n",
       " (1, 'resource'),\n",
       " (1, 'responded'),\n",
       " (1, 'same'),\n",
       " (1, 'scala,'),\n",
       " (1, 'script.'),\n",
       " (1, 'set'),\n",
       " (1, 'ship'),\n",
       " (1, 'ships'),\n",
       " (1, 'so).'),\n",
       " (1, 'so,'),\n",
       " (1, 'so..).'),\n",
       " (1, 'solution'),\n",
       " (1, 'some'),\n",
       " (1, 'spark-base:2.3.1:'),\n",
       " (1, 'spark-master:2.3.1:'),\n",
       " (1, 'spark-master|10.5.0.2'),\n",
       " (1, 'spark-submit.'),\n",
       " (1, 'spark-submit:2.3.1'),\n",
       " (1, 'spark-submit:2.3.1:'),\n",
       " (1, 'spark-worker'),\n",
       " (1, 'spark-worker-1|10.5.0.3'),\n",
       " (1, 'spark-worker-2|10.5.0.4'),\n",
       " (1, 'spark-worker-3|10.5.0.5'),\n",
       " (1, 'spark-worker:2.3.1:'),\n",
       " (1, 'spark-worker=3'),\n",
       " (1,\n",
       "  'spark.executor.extraJavaOptions=\\'-Dconfig-path=/opt/spark-apps/dev/config.conf\\'\"'),\n",
       " (1, 'spark://spark-master:7077)'),\n",
       " (1, 'state...'),\n",
       " (1, 'status'),\n",
       " (1, 'step,'),\n",
       " (1, 'steps'),\n",
       " (1, 'submit**'),\n",
       " (1, 'submitted'),\n",
       " (1, 'successful'),\n",
       " (1, 'support'),\n",
       " (1, 'testing'),\n",
       " (1, 'text](docs/spark-master.png'),\n",
       " (1, 'text](docs/spark-worker-1.png'),\n",
       " (1, 'text](docs/spark-worker-2.png'),\n",
       " (1, 'text](docs/spark-worker-3.png'),\n",
       " (1, 'thing'),\n",
       " (1, 'this:'),\n",
       " (1, 'three'),\n",
       " (1, 'topic)'),\n",
       " (1, 'toy'),\n",
       " (1, 'true'),\n",
       " (1, 'two'),\n",
       " (1, 'up'),\n",
       " (1, 'up*'),\n",
       " (1, 'us'),\n",
       " (1, 'useful'),\n",
       " (1, 'variables'),\n",
       " (1, 'volume'),\n",
       " (1, 'volumes'),\n",
       " (1, 'was'),\n",
       " (1, 'way'),\n",
       " (1, 'we'),\n",
       " (1, 'wich'),\n",
       " (1, 'wish'),\n",
       " (1, 'with(Optional)'),\n",
       " (1, 'worker-20180923151711-10.5.0.4-45381'),\n",
       " (1, 'workers,'),\n",
       " (1, '{'),\n",
       " (1, '}'),\n",
       " (2, '#App'),\n",
       " (2, '#Copy'),\n",
       " (2, '**spark-submit**'),\n",
       " (2, '--env'),\n",
       " (2, '/mnt/spark-apps'),\n",
       " (2, 'After'),\n",
       " (2, 'Master'),\n",
       " (2, 'Run'),\n",
       " (2, 'Submitting'),\n",
       " (2, 'application.'),\n",
       " (2, 'available'),\n",
       " (2, 'basically'),\n",
       " (2, 'code'),\n",
       " (2, 'configs'),\n",
       " (2, 'cpu'),\n",
       " (2, 'driver-20180923151753-0000'),\n",
       " (2, 'enough'),\n",
       " (2, 'files'),\n",
       " (2, 'first'),\n",
       " (2, 'from'),\n",
       " (2, 'if'),\n",
       " (2, 'images'),\n",
       " (2, 'input'),\n",
       " (2, 'installed'),\n",
       " (2, 'need'),\n",
       " (2, 'now'),\n",
       " (2, 'request'),\n",
       " (2, 'resources'),\n",
       " (2, 'sample'),\n",
       " (2, 'scala'),\n",
       " (2, 'see'),\n",
       " (2, 'shipped'),\n",
       " (2, 'simple'),\n",
       " (2, 'spark-worker-1'),\n",
       " (2, 'spark-worker-2'),\n",
       " (2, 'spark-worker-3'),\n",
       " (2, 'spark://spark-master:6066.'),\n",
       " (2, 'steps:'),\n",
       " (2, 'submit'),\n",
       " (2, 'successfully'),\n",
       " (2, 'test'),\n",
       " (2, 'validate'),\n",
       " (3, '/opt/spark-apps'),\n",
       " (3, '/opt/spark-data'),\n",
       " (3, '2'),\n",
       " (3, 'Validations'),\n",
       " (3, 'We'),\n",
       " (3, '```bash'),\n",
       " (3, '```sh'),\n",
       " (3, 'allocation'),\n",
       " (3, \"app's\"),\n",
       " (3, 'are'),\n",
       " (3, 'at'),\n",
       " (3, 'by'),\n",
       " (3, 'can'),\n",
       " (3, 'compose'),\n",
       " (3, 'copy'),\n",
       " (3, 'cores'),\n",
       " (3, 'cp'),\n",
       " (3, 'data'),\n",
       " (3, 'distributed'),\n",
       " (3, 'do'),\n",
       " (3, 'docker-compose'),\n",
       " (3, 'driver'),\n",
       " (3, 'environment'),\n",
       " (3, 'file'),\n",
       " (3, 'folder'),\n",
       " (3, 'image,'),\n",
       " (3, 'necessary'),\n",
       " (3, 'one'),\n",
       " (3, 'previously'),\n",
       " (3, 'standalone'),\n",
       " (3, 'step'),\n",
       " (3, 'submission'),\n",
       " (3, 'these'),\n",
       " (3, 'use'),\n",
       " (3, 'workers'),\n",
       " (3, \"workers's\"),\n",
       " (4, '![alt'),\n",
       " (4, '\"Spark'),\n",
       " (4, '###'),\n",
       " (4, '3'),\n",
       " (4, 'Docker'),\n",
       " (4, 'UI\")'),\n",
       " (4, '\\\\'),\n",
       " (4, 'an'),\n",
       " (4, 'containers.'),\n",
       " (4, 'default'),\n",
       " (4, 'each'),\n",
       " (4, 'in'),\n",
       " (4, 'jar'),\n",
       " (4, 'or'),\n",
       " (4, 'this'),\n",
       " (4, 'with'),\n",
       " (5, '1'),\n",
       " (5, '15:17:53'),\n",
       " (5, ':'),\n",
       " (5, 'RAM'),\n",
       " (5, 'based'),\n",
       " (5, 'have'),\n",
       " (5, 'into'),\n",
       " (5, 'just'),\n",
       " (5, 'spark-submit'),\n",
       " (6, '&'),\n",
       " (6, '-'),\n",
       " (6, '-l'),\n",
       " (6, '-ti'),\n",
       " (6, '2018-09-23'),\n",
       " (6, 'I'),\n",
       " (6, 'INFO'),\n",
       " (6, 'RestSubmissionClient:54'),\n",
       " (6, 'Spark'),\n",
       " (6, 'This'),\n",
       " (6, 'Worker'),\n",
       " (6, '```'),\n",
       " (6, 'all'),\n",
       " (6, 'as'),\n",
       " (6, 'be'),\n",
       " (6, 'create'),\n",
       " (6, 'created'),\n",
       " (6, 'exec'),\n",
       " (6, 'following'),\n",
       " (6, 'ls'),\n",
       " (6, 'running'),\n",
       " (6, 'using'),\n",
       " (7, 'run'),\n",
       " (8, '##'),\n",
       " (8, 'A'),\n",
       " (8, 'app'),\n",
       " (8, 'application'),\n",
       " (8, 'master'),\n",
       " (8, 'used'),\n",
       " (8, 'will'),\n",
       " (9, 'cluster'),\n",
       " (9, 'image'),\n",
       " (9, 'of'),\n",
       " (9, 'worker'),\n",
       " (10, 'The'),\n",
       " (10, 'for'),\n",
       " (10, 'make'),\n",
       " (10, 'on'),\n",
       " (10, 'you'),\n",
       " (12, '#'),\n",
       " (13, 'and'),\n",
       " (14, 'is'),\n",
       " (15, 'docker'),\n",
       " (17, 'your'),\n",
       " (18, 'a'),\n",
       " (20, '*'),\n",
       " (27, 'spark'),\n",
       " (31, 'to'),\n",
       " (34, 'the'),\n",
       " (104, '')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#key\n",
    "rdd2.sortBy(ascending=True,numPartitions=None, keyfunc = lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python\n",
    "#rdd2.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 106, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'tuple' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'tuple' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-98f7e5ac2508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampleStdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msampleStdev\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \"\"\"\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampleStdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msampleVariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 106, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'tuple' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'tuple' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "#rdd3 = rdd2.sampleStdev()\n",
    "rdd2.saveAsTextFile(\"/opt/data/1/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
